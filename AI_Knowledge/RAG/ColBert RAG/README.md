# ColBERT (Contextualized Late Interaction over BERT): [article](https://github.com/stanford-futuredata/ColBERT)

### The Core Idea: Less-Lossy Encoding with Summed Similarity

At its heart, ColBERT does two main things differently from traditional retrieval models:

1. **Less-Lossy Encoding:** Instead of compressing an entire document into a single vector, ColBERT keeps separate embeddings for each token in the document. These are stored in a matrix.
2. **Summed Similarity Scoring:** When matching a query to documents, ColBERT simply sums up the similarities between query tokens and document tokens.

[![](https://github.com/stanford-futuredata/ColBERT/raw/main/docs/images/ColBERT-Framework-MaxSim-W370px.png)](https://github.com/stanford-futuredata/ColBERT/blob/main/docs/images/ColBERT-Framework-MaxSim-W370px.png)

### Why is this Powerful?

1. **Preserving Information**

By keeping token-level embeddings, ColBERT preserves much more of the original document’s information. Think of it like keeping a high-resolution image instead of a thumbnail. This allows for more nuanced matching later on.

2. **Flexible Matching**

The summed similarity approach allows each part of the query to match with any part of the document. This flexibility can capture relevance even when the query and document use different phrasing or word order.

### How it Works: A simple Explanation

1. **Encoding:**
   — **Documents**: Each token in a document gets its own embedding vector.
   — **Queries**: Similarly, each token in a query gets its own embedding.
2. **Retrieval**:
   — For each query token, find its similarity with every document token.
   — Take the maximum similarity for each query token.
   — Sum these maximum similarities.
   — The documents with the highest sums are retrieved.
3. **Usage in RAG (Retrieval-Augmented Generation):**
   — Once documents are retrieved, they’re used as additional context, just like in other RAG systems.

![](https://miro.medium.com/v2/resize:fit:700/1*zZqJiAqGG-ReMVFHj6fgkg.png)

### Why is it that efficient?

As seen in the above figure (d), by isolating the encoding procedure of document and query, it’s possible to pre-compute document encodings offline, thereby reducing computational load per query significantly.

It’s observed that the fine-grained matching of interaction-based models and the pre-computation of document representations of representation-based models can be combined by retaining yet judiciously delaying the query–document interaction. This delaying procedure reduces the computational overhead by a significant margin, thus making the retrieval process swift.

Talking about numbers, it delivers over 170 times speedup relative to other BERT-based retrieval models, while maintaining the overall performance.

### Detailed explanation

A pre-trained Embedding matrix is used to generate tokens for q and d. Different kinds of tokenization methods can be used, WordPiece tokenisation being the default one. We can also use SentencePiece tokenisation, Byte-level tokenisation, n-gram tokenisation etc.

These tokens are separately passed into BERT-based models for generating encoded representation. Let ‘Eq’ and ‘Ed’ be the contextualised encoding generated by the BERT models separately. There is a ‘model’ attribute to change the base model, by default it’s “bert-base-uncased”.

1. **Embedding Generation** :

* Both the ****query** and** **documents** (or passages) are passed through BERT.
* BERT generates a sequence of ****token embeddings** for both the query and the document. These embeddings are** **contextualized,** meaning each token is represented based on its surrounding tokens.
* For example, if the query is "What is ColBERT?" and the document contains "ColBERT is a retrieval model," BERT will produce different embeddings for "ColBERT" in both contexts based on the words around it.

2. **Late Interaction** :

* ColBERT performs ****late interaction** by computing the similarity between** ****each query token embedding** and** **each document token embedding** . Specifically, it uses a **MaxSim** function to compute the maximum similarity between every query token and the most relevant token from the document.
* This allows for a **fine-grained matching** between query and document tokens while avoiding full interaction between the entire embeddings, which can be computationally expensive.
* This interaction provides a balance between capturing detailed, token-level relevance and efficiency during retrieval.

![](https://miro.medium.com/v2/resize:fit:1400/1*Jes8Rb31kByKuwkTsqORHA.jpeg)


### **Advantages of Using BERT in ColBERT for RAG** :

* **Contextualized Retrieval** : BERT’s deep contextual embeddings allow ColBERT to understand the meaning of query terms based on their context, leading to more relevant document retrieval compared to traditional keyword-based or bag-of-words approaches.
* **Efficiency with Late Interaction** : ColBERT reduces computational costs by performing **late interaction** between query and document token embeddings, making it scalable even for large document collections.
* **Enhanced Relevance** : The fine-grained token-level matching provided by ColBERT’s MaxSim function captures deeper, token-specific interactions between queries and documents, improving retrieval quality.

## Training ColBERT: [article](https://medium.com/@varun030403/colbert-a-complete-guide-1552468335ae)

![](https://miro.medium.com/v2/resize:fit:646/1*7klydyPCSxn6YIQFFpytPA.png)

**ColBERT is trained on triplets which are as follows :**
*<query, positive_document, negative_document>*


***a) query:** Query for which we want to retrieve a document.*

***b) positive_document:** Document which is relevant to the query and can plausibly contain the answer to the query.*

***c) negative_document:** Document which is not relevant to the query and can’t plausibly contain the answer to the query.*


### Example ways to train the model:

a) We initially use a naive retrieval model for ranking the documents based on the heuristic algorithm of that model, we generally use the BM-25 model as the naive retrieval model. It uses tf-idf technique to rank the documents. Then this existing retrieval model is used to collect the top-k passages for every training query and, with a simple heuristic, sort these passages into positive (+ve) and negative (–ve) examples, using those to train another, more effective retriever. This process is applied thrice, resulting in a robust trained ColBERT model.

b) We get the triplets by using a naive retriever. The top-k ranked documents are the pos_documents, and the rest of them are neg_documents. ‘k’ is the hyperparameter, whose value can be adjusted accordingly. We use these triplets to again train the ColBERT in the same fashion. This process is repeated 3–5 times and we finally get a trained ColBERT model.

## Articles

* [Article](https://thenewstack.io/overcoming-the-limits-of-rag-with-colbert/): Overcoming the Limits of RAG with ColBERT
* [Article](https://medium.com/@mbonsign/colbert-explained-simple-ideas-behind-powerful-retrieval-8ccbdeea178b): ColBERT Explained
* [Article](https://medium.com/@varun030403/colbert-a-complete-guide-1552468335ae): ColBERT: A complete guide
* [Video](https://www.youtube.com/watch?v=xTzUn3G9YA0): ColBERT with RAGatouille practical example in Google Collab Notebook
* [Repo](https://github.com/stanford-futuredata/ColBERT): ColBERT (v2) Github Repository
