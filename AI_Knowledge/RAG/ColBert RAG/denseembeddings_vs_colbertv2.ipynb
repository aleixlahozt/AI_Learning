{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Embeddings vs ColBERTv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file import PDFReader\n",
    "\n",
    "loader = PDFReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Prithvi WxC: Foundation Model for Weather and Climate\\nJohannes Schmude1,†,‡,Sujit Roy2,7,†,‡,Will Trojak1,Johannes Jakubik1,\\nDaniel Salles Civitarese1,Shraddha Singh1,Julian Kuehnert1,Kumar\\nAnkur2,Aman Gupta3,Christopher E Phillips2,Romeo Kienzler1,Daniela\\nSzwarcman1,Vishal Gaur2,Rajat Shinde2,Rohit Lal2,Arlindo Da Silva6,\\nJorge Luis Guevara Diaz1,Anne Jones1,Simon Pfreundschuh4,Amy Lin2,\\nAditi Sheshadri3,Udaysankar Nair2,Valentine Anantharaj5,Hendrik\\nHamann1,Campbell Watson1,Manil Maskey7,Tsengdar J Lee8,Juan\\nBernabe Moreno1,Rahul Ramachandran7\\n†Equal Contribution,\\n‡Johannes.Schmude@ibm.com , Sujit.Roy@nasa.gov*\\nABSTRACT\\nTriggered by the realization that AI emulators can rival the performance of tra-\\nditional numerical weather prediction models running on HPC systems, there is\\nnow an increasing number of large AI models that address use cases such as fore-\\ncasting, downscaling, or nowcasting. While the parallel developments in the AI\\nliterature focus on foundation models – models that can be effectively tuned to\\naddress multiple, different use cases – the developments on the weather and cli-\\nmate side largely focus on single-use cases with particular emphasis on mid-range\\nforecasting. We close this gap by introducing Prithvi WxC, a 2.3 billion parameter\\nfoundation model developed using 160 variables from the Modern-Era Retrospec-\\ntive Analysis for Research and Applications, Version 2 (MERRA-2). Prithvi WxC\\nemploys an encoder-decoder-based architecture, incorporating concepts from var-\\nious recent transformer models to effectively capture both regional and global\\ndependencies in the input data. The model has been designed to accommodate\\nlarge token counts to model weather phenomena in different topologies at fine\\nresolutions. Furthermore, it is trained with a mixed objective that combines the\\nparadigms of masked reconstruction with forecasting. We test the model on a\\nset of challenging downstream tasks namely: Autoregressive rollout forecasting,\\nDownscaling, Gravity wave flux parameterization, and Extreme events estimation.\\nThe pretrained model with 2.3 billion parameters, along with the associated fine-\\ntuning workflows, has been publicly released as an open-source contribution via\\nHugging Face.\\n1 I NTRODUCTION\\nDeep learning is increasingly transforming weather applications by delivering highly accurate fore-\\ncasts with reduced computational costs compared to traditional numerical weather prediction meth-\\n*1IBM Research;2Earth System Science Center, The University of Alabama in Huntsville, Huntsville, AL,\\nUSA;3Department of Earth System Science, Stanford University, Stanford, USA;4Department of Atmospheric\\nScience, Colorado State University, Fort Collins, CO, USA;5National Center for Computational Sciences / Oak\\nRidge National Laboratory;6NASA/Goddard Space Flight Center, USA;7NASA Marshall Space Flight Center,\\nHuntsville, AL, USA;8NASA Headquarters, Washington, DC, United States of America\\n1arXiv:2409.13598v1  [cs.LG]  20 Sep 2024',\n",
       " 'ods (Bi et al., 2023; Lam et al., 2023; Mukkavilli et al., 2023). Unlike the traditional physics-based\\napproaches, deep learning models do not directly simulate the underlying physics. Instead, they\\ncapture this through probability distributions derived from model training, a method adapted from\\nnatural language processing and computer vision. This technique has proven surprisingly effec-\\ntive in approximating complex physical systems such as the weather. However, most current deep\\nlearning models for weather are task-specific forecast emulators, which focus solely on the forecast-\\ning problem. (See, however, Koldunov et al. (2024).) Key examples include FourCastNet (Pathak\\net al., 2022), Pangu (Bi et al., 2022), GraphCast (Lam et al., 2022), FengWu (Chen et al., 2023),\\nStormer (Nguyen et al., 2023b) and AIFS (Lang et al., 2024). Machine learning models also show\\npromise for longer-term subseasonal-to-seasonal forecasts (Weyn et al., 2021). Additionally, ML-\\nbased approaches are being explored to enhance climate predictions (see Mansfield et al., 2023;\\nEyring et al., 2024, for a review), with a focus on the development of ML-driven parameterizations\\n(Rasp et al., 2018; Zhao et al., 2019; Espinosa et al., 2022; Yuval & O’Gorman, 2023; Henn et al.,\\n2024; Gupta et al., 2024), bias corrections (Bretherton et al., 2022; Gregory et al., 2024), and assess-\\nments of climate change impacts (Davenport & Diffenbaugh, 2021; Diffenbaugh & Barnes, 2023,\\namong others). There is fascinating emerging work that combines the strengths of the data-driven\\nand physics-based approaches (Kochkov et al., 2024; Husain et al., 2024; Roy et al., 2024). Fi-\\nnally, there are further large, task-specific models for Nowcasting (Andrychowicz et al., 2023) and\\ndownscaling (Mardani et al., 2024).\\nLooking beyond atmospheric sciences at developments in AI in general and language models in\\nparticular, the last few years have been dominated by the emergence of foundation models. That\\nis, large AI models pretrained in a task-agnostic manner that can be effectively fine-tuned to ad-\\ndress a number of specific use cases. Despite the mirroring successes of large AI models in both\\nfields, applications of the foundation model principle to atmospheric sciences have been rare. At-\\nmoRep (Lessig et al., 2023) considered problems ranging from nowcasting to downscaling and bias\\ncorrections; Aurora (Bodnar et al., 2024) focusses a number of different forecasting problems.\\nTo address this gap, we introduce Prithvi WxC, a large-scale foundation model for weather and\\nclimate applications trained on 160 atmospheric variables from the Modern-Era Retrospective anal-\\nysis for Research and Applications, Version 2 (MERRA-2) data set. MERRA-2 is a widely-used\\nreanalysis dataset from NASA providing global atmospheric data, including temperature, humidity,\\nand wind. Spanning from 1980 to the present day with spatial resolution of 0.5 by 0.625 degrees\\nand temporal resolution of 3 hours (Gelaro et al., 2017), it is valuable for climate research and\\natmospheric studies.\\nPrithvi WxC is a transformer-based deep learning architecture which combines ideas from several\\nrecent transformer architectures in order to effectively process regional and global dependencies of\\nthe input data and to efficiently process longer sequence lengths of tokens. This allows the model to,\\nfor example, run in different spatial contexts or infuse additional tokens from off-grid measurements\\nto the model during finetuning. We additionally experiment with different loss functions, for ex-\\nample, by removing task-specific temporal variances from loss functions of forecast emulators and\\nreplacing them with task-agnostic climatology variances.\\nThe validation of Prithvi WxC extends from zero shot evaluations for reconstruction and forecasting\\nto downstream tasks such as downscaling of weather and climate models, the prediction of hurricane\\ntracks and atmospheric gravity wave flux parameterization.\\n2 P RITHVI WXC\\nFrom an AI perspective, Prithvi WxC has been designed to address several questions that arise when\\nconsidering the meaning of foundation models for atmospheric physics: Since weather models can\\nrun on the entire earth or in a regional context, do we need specialized architectures for global and\\nlocal problems? Do we need to differentiate between models with zero and non-zero lead time? If\\nwe do consider tasks with zero and non-zero lead time, what is a suitable pretext task for pretraining?\\n2.1 P RETRAINING OBJECTIVE\\nAs outlined above, the most celebrated successes at the intersection of AI and atmospheric sciences\\nconcern forecast emulators . I.e. models that are given the state of the atmosphere at times tand\\n2',\n",
       " 't−δtpredict the state at t+δt. While forecasting is a obvious task for weather and climate data,\\nthe prototypical approach in the computer vision literature is that of the masked autoencoder (MAE)\\n(He et al., 2022). There are several considerations that make masking attractive for pretraining:\\nTo start, while both NWP as well as reanalysis data is gridded and dense, observational data is\\nungridded and sparse. As such, it might not be surprising that the emerging literature on models\\nworking directly on observation makes heavy use of masking (Vandal et al., 2024; McNally et al.,\\n2024). On a related note, the forecasting objective becomes trivial in the case of δt= 0. At the\\nsame time there are use cases such as downscaling or data assimilation for which such a time step\\nis meaningful. Thus, a foundation model aiming to address all such use cases should be able to deal\\nwith a non-positive forecast step. On the more technical side, a common problem in this space is\\nthe size of the data. As noted in the original work (He et al., 2022), masking is highly memory\\nefficient. As long as masking is implemented without additional masking tokens, the technique\\nreduces memory pressure and increases training speeds.\\nTo our knowledge, only (Lessig et al., 2023; Schmude & Nathaniel, 2023) used masking on reanaly-\\nsis data. The latter did so in the context of contrastive learning while the former used a 3D masking\\napproach akin to (Feichtenhofer et al., 2022). However, given the success of the forecasting objec-\\ntive and in order to avoid holding a 3D data cube in GPU memory, we merge these objectives slightly\\ndifferently. Effectively we train a 2D model with masking for which the output is a prediction:\\nˆXt+δt=fθ[M0.5(Xt, Xt−δt)]. (1)\\nHere Xtis the data, ˆXta prediction, fθa neural network and M0.5the masking operator for 50%.\\nNow, given that we are training a foundation model, we are aiming for easy generalizability. Thus,\\nwe allow for different values for δtfor inputs and the target. A variable lead time appeard already in\\nNguyen et al. (2023b). Moreover, we also use some static data such as elevation, land fraction etc.\\nSee 2.2 for details.\\nThe objective of equation 1 can be improved on. Indeed, the forecast emulators of (Lam et al., 2023;\\nNguyen et al., 2023b) do not predict Xt+δtbut the tendency Xt+δt−Xt. (See however (Lang et al.,\\n2024) as well as (Bodnar et al., 2024) which both output absolute quantities.) The rationale seems\\nclear: A model that predicts tendencies gets the performance of a persistence forecast for free and\\none does not spend a lot of GPU cycles learning known biases. In our case however the objective\\nwould again become trivial for δt= 0.\\nIn light of this we turn to another source of free information: Climatology. That is, instead of\\npredicting the difference from the current time stamp, we model the deviation from historical climate\\nat this time, Ct. All in all, our pretraining objective is\\nˆXt+δt−Ct+δt\\nσC=fθ\\x14\\nM0.5\\x12Xt−µ\\nσ,Xt−δτ−µ\\nσ\\x13\\n;Ct+δt−µ\\nσ, S, δt, δτ\\x15\\n. (2)\\nHere, µandσare per parameter means and standard deviations (computed across space and time).\\nσ2\\nC=σ2\\nC(Xt−Ct)is the variance of the historical anomaly; again comptued aross space and time.\\nSare static inputs and δtandδτare the time steps for the target and the inputs respectively.\\n2.2 D ATA\\n2.2.1 MERRA-2\\nThe Modern-Era Retrospective Analysis for Research and Applications Version 2 (MERRA-2)\\n(Gelaro et al., 2017), developed by NASA’s Global Modeling and Assimilation Office (GMAO),\\nserves as the primary dataset for this study. It uses a cubed-sphere grid, which results in uniform\\ngrid spacing at all latitudes. This design minimizes grid spacing irregularities found in latitude-\\nlongitude grids, enhancing the dataset’s spatial consistency and usefulness for global-scale analyses.\\nMERRA-2 provides a comprehensive and consistent record of Earth’s climate and atmospheric con-\\nditions, offering valuable insights into long-term climate trends and variability. It is a state-of-the-art\\nreanalysis dataset that integrates a range of observational data with advanced modeling techniques\\nto produce a high-quality, multidecadal record of atmospheric conditions (Rienecker et al., 2011;\\nGelaro et al., 2017). It is particularly useful for climate research due to its extensive historical\\ncoverage and sophisticated data assimilation methods.\\n3',\n",
       " 'The dataset includes variables at model native levels corresponding to nominal pressure surfaces\\nwhich are 985 hPa, 970 hPa, 925 hPa, 850 hPa, 700 hPa, 600 hPa, 525 hPa, 412 hPa, 288 hPa,\\n245 hPa, 208 hPa, 150 hPa, 109 hPa, and 48 hPa, with data available every 3 hours. Variables\\nat these levels include wind components (U, V), vertical wind ( ω), air temperature (T), specific\\nhumidity (QV), actual mid-level pressure (PL), and mid-layer geopotential height (H), cloud fraction\\n(CLOUD), cloud massk fraction that is ice (QI) and water (QL).\\nAdditional single-level variables are available at 1-hour intervals and include near-surface wind\\ncomponents (U10, V10), near-surface (2 meter) air temperature (T2M), skin temperature (TS), sur-\\nface roughness (Z0M), specific humidity (QV2M), surface pressure (PS), sea level pressure (SLP),\\ncolumn-total ice, liquid water and water vapor (TQI, TQL, TQV), longwave radiation emitted by\\nthe surface (LWGEM), longwave radiation absorbed by the surface (LWGAB), upward longwave at\\nthe top of atmosphere (LWTUP), net downward shortwave radiation at the surface (SWGNT) and\\nnet shortwave at top of atmosphere (SWTNT). Static variables include surface geopotential height\\n(PHIS), land fraction (FRLAND), ocean fraction (FROCEAN), and ice fraction (FRACI), which are\\nused to provide essential static information, and is varying in space, but not time. Time-averaged\\nvariables, such as rootzone soil wetness (GWETROOT), leaf area index (LAI), and surface fluxes\\n(EFLUX, HFLUX), are aggregated from 1-hourly intervals, because these are the diagnostics vari-\\nables and not available at the analysis time. Aggregation methods are used for variables from hourly\\nproducts, where means of adjacent hourly values are used to create 12:00 UTC data. For example,\\nthe mean of 11:30 and 12:30 values is calculated to prepare the 12:00 UTC data. Missing values\\n(NaNs) in GWETROOT and LAI are replaced with 1 and 0, respectively, to maintain data avail-\\nability over the ocean. Static datasets are incorporated by creating monthly files, ensuring that the\\nstatic variables (PHIS, FRLAND, FROCEAN, FRACI) remain consistent for each month, thereby\\nmaintaining the integrity of static information throughout the dataset. List of variables used in the\\ntraining is listed in the Appendix tables 2, 3 and 4. We train the model using data from 1980 to 2019.\\nWe validate with data from one of the years in the 2020-2023 range, depending on task.\\n2.2.2 C LIMATOLOGY\\nThe climatology appearing in equation 2 was computed from 20 years of MERRA-2 data following\\nthe methodology of the ERA-Interim climatology (Janou ˇsek, 2011). That is, for each Julian day and\\neach hour of the day we aggregate all data across the last 20 years. Subsequently we apply a 61-day\\nrolling window weighted average to this. The weights are given by a second order polynomial. Thus\\nthe climatology resolves the day-night cycle. There are 365×8timestamps and each pixel is based\\non20×61 = 1220 data points. We used the same 20 year period that we used for training; that is\\n1980-2019.\\n2.2.3 N ORMALIZATION\\nWhile equation 2 is a fairly natural training objective, we found that leaving the normalization\\nconstants σandσCunconstrained leads to instabilities during training. This is essentially due to the\\nlarge range of values we have, especially the anomalies in the mass fraction of cloud liquid water\\nQL at high model levels can be as small as 10−26at level 34. To avoid such extreme values upsetting\\nnumerics, we impose 10−4≤σ≤104and similarly 10−7≤σC≤107. In both cases, this mainly\\naffects QIandQLat high levels.\\n2.3 A RCHITECTURE\\n2.3.1 A SCALABLE AND FLEXIBLE VISION TRANSFORMER\\nAt it’s core, Prithvi WxC is a scalable and flexible 2D vision transformer. To keep it as flexible as\\npossible, we aimed not to use architecture elements that restrict to “rectangular” topologies for data.\\n(Even though we train on MERRA-2 data on a rectangular lat/lon grid, one can envision training or\\nrunning inference directly on Gaussian grids.) Vanilla ViTs would satisfy this requirement, yet do\\nnot scale to large token counts. Considering the different flavors of scalable transformers we notice\\nthe findings of “Hiera” (Ryali et al., 2023). Here the authors show that it is possible to surpass the\\nperformance of Swin transformers with a more flexible and simplified architecture. Turning back\\nto AI models for weather, Andrychowicz et al. (2023) made use of MaxViT (Tu et al., 2022) which\\nleverages axial attention (albeit with convolutions). In the end, our core idea is that if we pretrain\\n4',\n",
       " 'the model using only attention, we keep the core of the model flexible and can add convolutions\\nat fine-tuning time to increase performance when suitable. We do so by joining the approaches of\\nHiera and MaxViT.\\ntranspose token window\\nlocal masking\\nglobal maskingXt,Xt−j\\n Xt+i\\nOutput projectionreconstruct\\nbatch\\n...Encoder\\nGlobal attention\\nalong global\\nsequenceLocal attention\\nalong local\\nsequenceLocal attention\\nalong local\\nsequenceN x local-global blocks\\nDecoderLocal attention\\nalong local\\nsequenceGlobal attention\\nalong global\\nsequenceLocal attention\\nalong local\\nsequence\\nM x local-global blocks\\nFigure 1: Prithvi WxC core architecture elements and masking scheme. For simplicity the figure\\nignores elements such as embedding and output layers as well as position encodings.\\nIn detail, the only constraint we impose on the data is the ability to structure tokens into windows\\n– akin to Swin, Hiera and MaxViT. Doing so, our data takes the shape (batch, windows, tokens,\\nfeatures), where the second dimension enumerates windows and the third tokens within each win-\\ndow. Subsequently we alternate attention within a window and across windows. The latter is similar\\nto (Tu et al., 2022): Modulo masking the nth token in each window interacts with the nth token\\nin all other windows. This is easily implemented by transposing the window and token dimension\\nbetween transformer layers. Attention acts on the 3rddimension with the second being rolled into\\nthe batch dimension. In what follows we will refer to attention within a window as “local” and at-\\ntention across windows as “global”. When masking, we can either mask out entire global windows\\nor individual tokens within a window. A byproduct of the latter is that global attention no longer\\nconnects the same token in each window. E.g. if token 1 in window 1 is masked, then token 2 in\\nwindow 1 connects to token 1 in window to etc. For an illustration of the attention pattern as well as\\nthe overall encoder/decoder architecture see figure 1.\\n2.3.2 O VERALL MODEL ARCHITECTURE\\nAs shown in equation 2, the model has several inputs: To start, there are the dynamic inputs Xt,\\nXt−δτfrom the MERRA-2 re-analysis. These take the shape T×[PS+(PV×L)]×H×W. Here,\\nTis time, fixed to 2. PSare the 20 surface parameters and PVthe 10 vertical parameters at L= 14\\nvertical model levels for a total of 160 inputs. HandWdenote latitude and longitude respectively.\\nSince we flatten all temporal and parameter dimensions, the dynamic model input comprises a data\\ncube of dimension 320×360×576. Climatological inputs Ct+δτtake the shape 160×360×576\\nas there is the same number of parameters yet no time dimension. The static inputs are based on 4\\nstatic parameters from MERRA-2 – elevation, land cover, ice cover, lake cover – as well as cosine\\nand sine of day of year and hour of the day.\\nWe use a static Fourier position encoding that respects the periodicity of the earth. In addition, there\\nis a learned encoding for both the lead time δtas well as the input time step δτ. An earlier version\\nof the model used a non-spatiotemporal context token to communicate information such as the lead\\n5',\n",
       " 'time. However, this led to the emergence of specialized transformer layers that paid heavy attention\\nto this token, which is in conflict with stochastic depth (drop path) which we enabled during the\\nscaling phase. Finally there are separate linear embedding layers for the dynamic inputs as well as\\nthe concatenation of the climatological and static ones. Once embedded, all tokens are added up.\\n2.4 P RETRAINING\\n2.4.1 S CALING\\nIn its final configuration, Prithvi WxC comprises 25 encoder and 5 decoder blocks. As both the\\nencoder and decoder start and end with local attention, 13 (3) of these blocks perform local and 12\\n(2) global attention respectively. The internal dimension is 2,560. With 16 attention heads and an\\nMLP multiplier of 4 this results in 2.3 billion parameters. We use a token size of 2 by 2 pixel. Each\\nwindow measures 30 by 32 pixel or 15 by 16 tokens. With these choices we are dealing with 51,840\\ntokens per sample yet are keeping the length of the global and local sequence roughly balanced.\\nNote that both token and window size can be changed when tuning the model and we will do so\\nrepeatedly below. In either case, with these choices, the model consumes a bit more than 43 GB of\\nGPU memory in pretraining. If we keep the masking at 50% we are able to backpropagate through 4\\nautoregressive steps on a 80 GB A100. (Masking only applies to the first autoregressive step.) If the\\ndata becomes dense (i.e. 0% masking) this reduces to 3 steps. Since the data becomes dense in the\\ndecoder and our pretraining data does live on a rectangular grid, we add a Swin-shift to the decoder\\nlayers. The overall scale was chosen to ensure that autoregressive “rollout” training is still possible.\\nTo bring the model to this scale, we make use of Fully Sharded Data Parallelism (FSDP) as well\\nas flash attention (via scaled dot product attention). We train the model with bfloat16 precision.\\nHowever, to ensure numeric stability we only use bfloat16 for the transformer layers. The input and\\noutput layers remain at float32. Finally, we use activation checkpointing.\\n2.4.2 P RETRAINING PROTOCOL\\nWe train Prithvi WxC in two phases. The first phase uses 5% drop path, a 50% masking ratio\\nand alternates “local” and “global” masking from gradient descent step to gradient descent step.\\nMoreover, for each sample we select a random forecast lead time (among 0, 6, 12 and 24 hours\\nahead) as well as a random delta between inputs (-3, -6, -9, -12). With this randomization, we train\\nthe model on 64 A100 GPUs and batch size 1 for 100,000 gradient descent steps. After 2,500 steps\\nof linear warm-up we perform cosine-annealing from 10−4to10−5. This results in a highly flexible\\nmodel that we use for our downscaling and gravity wave parametrization experiments as well as for\\nthe zero-shot reconstruction evaluations.\\nTo further attune the model to forecasting applications, we make a few changes: We reduce the\\nmasking ratio to 0% and add a Swin-shift to the encoder. Also, we set drop path to 0%. In addition,\\nwe fix both the forecast lead time and input delta to six hours so that there is no more randomiza-\\ntion. Keeping the learning rate constant at 10−5, we tune the model with 1, 2 and 3 autoregressive\\nsteps on a varying compute footprint ranging from 16 to 48 GPUs. In this phase we also modify\\nthe training objective equation 2 by using additional weights. For the vertical parameters, weights\\ndepend linearly on pressure level (in hPa). In addition, we weight H, ω, T, U and V with 1 yet cloud,\\nPL, QI, QL with 0.1. For the surface parameters, we weigh u10m and v10m with 1, SLP and t2m\\nwith 0.1 and the remaining parameters with 0.01. Essentially this follows (Lam et al., 2022) with the\\nexception that we found it beneficial to swap the weights for t2m and u10m as well as v10m while\\nsuppressing all variables which are not standard in the AI-forecast emulation literature by a factor of\\nten. This version of the model is used for the forecast evaluation as well as the hurricane-forecasting\\nuse case.\\n2.5 Z ERO-SHOT VALIDATION\\n2.5.1 M ASKED RECONSTRUCTION\\nTo understand the model’s zero shot performance, we can consider two sets of metrics: Reconstruc-\\ntion and forecasting. Figure 2 shows two examples of the former. Note that the model is capable\\nof reconstructing atmospheric state from as little as 5% of the original data when the samples are\\n6',\n",
       " 'u10m95% masked input reconstruction ground truth\\ntqv75% masked input reconstruction ground truth15\\n10\\n5\\n051015\\nt2m [K]\\n0.017.535.052.570.0\\ntqv [kg/m**2]Figure 2: Zero-shot reconstruction performance with Prithvi WxC. The first row shows “local”\\nmasking where we mask 95% of individual tokens. The second row shows “global” masking where\\nwe mask 75% of attention windows. At these extreme masking ratios some fine structure is lost in\\nthe reconstruction. See figure 3 for metrics.\\nstill relatively dense and 25% when we mask out large areas. Figure 3 shows RMSE scores against\\nmasking ratios for both masking strategies and 0 as well as 6 hours ahead. (The 6-hour metrics\\nhere are obtained without the additional rollout tuning phase.) It is interesting that reconstruction\\nperformance is relatively little affected by lead time at the lower end of masking ratios. This opens\\nup the possibility of initializing a forecast model with randomly sampled tokens to obtain an en-\\nsemble forecast as well as the future research direction to fine-tune the model to integrate sparse\\nobservational data.\\n0.51.01.52.02.5RMSEt2m [K]\\n123t at 850 hPa [K]\\n123t at 525 hPa [K]\\n123RMSEu10m [m/s]\\n24u at 850 hPa [m/s]\\n2468u at 525 hPa [m/s]\\n60 80 100\\nMasking ratio [%]246RMSEtqv [kg/m**2]\\n60 80 100\\nMasking ratio [%]510h at 850 hPa [m]\\n60 80 100\\nMasking ratio [%]02040h at 525 hPa [m]\\n0h, local 0h, global 6h, local 6h, global\\nFigure 3: Zero-shot reconstruction performance of Prithvi WxC evaluated with 50, 60, 70, 80, 90,\\n95 and 99% masking. Note that the 6-hour ahead values are without any forecast tuning.\\n7',\n",
       " '2.5.2 F ORECASTING\\nTo understand the zero-shot forecasting performance of the model, we perform autoregressive fore-\\ncasts with dense data up to 5 days ahead. See figure 4. To put the performance into context, we\\ncompare data from various AI forecast emulators as well as the ECMWF IFS as provided by Weath-\\nerBench2 (Rasp et al., 2024). Some care has to be taken when interpreting these results. Weath-\\nerBench2 compares against ERA5 and the IFS Analysis at 0.25 degrees resolution while we work\\nwith MERRA-2 at 0.5 by 0.625. Moreover, our model generates a number of forecasts for which no\\nreference AI prediction exists. Most notably the “cloud” variables.\\nWith all these caveats in mind, Prithvi WxC performs well to exceptionally well at very short lead\\ntimes (6 and 12 hours), particularly for parameters like surface temperature. However, performance\\nthen decays and after about 66 hours Prithvi WxC falls below the performance of Pangu.\\nThe reader might remark that we should not refer to this as zero shot performance when the model\\nhas gone through rollout tuning. However, we expect that one should do several things when truly\\npushing for maximal forecasting performance. Among these are adding additional convolutional or\\nneural operator layers that improve information flow from attention window to attention window as\\nwell as deeper rollout tuning.\\nFinally, one might want to speculate whether the strong performance at shortest lead times is re-\\nlated to the masking objective, a consideration that should be of interest for nowcasting and data\\nassimilation objectives.\\n0.000.250.500.751.001.251.501.75RMSEt2m [K]\\n0.00.51.01.52.02.53.0u10m [m/s]\\n12345678h at 850 hPa [m]\\n0 24 48 72 96 120\\nLead time [h]0.00.51.01.52.0RMSEt at 850 hPa [K]\\n0 24 48 72 96 120\\nLead time [h]01234u at 850 hPa [m/s]\\n0 24 48 72 96 120\\nLead time [h]0.080.100.120.14cloud at 850 hPa [0-1]\\nPrithvi WxC IFS HRES IFS ENS (mean) GraphCast Pangu-Weather\\nFigure 4: Zero-shot forecasting performance of Prithvi WxC.\\n2.5.3 H URRICANE TRACK FORECASTING\\nWe have validated Prithvi WxC to assess its capability in forecasting the formation, dissipation,\\nintensification, and tracking of hurricanes ranging from Category 3 to Category 5, formed over the\\nAtlantic Ocean between 2017 and 2023. The list of hurricanes used in the analysis is provided in\\nTable 6. The performance of the model was benchmarked against observed hurricane tracks from\\nthe HURDAT database and two other models: FourCastNet trained on MERRA-2, and FourCastNet\\ntrained on the ERA5 dataset. One significant example is Hurricane Ida, a Category 4 storm that\\nstruck Louisiana in 2021. This hurricane, the second-most damaging in Louisiana’s history after\\n8',\n",
       " 'Hurricane Katrina, is presented as a sample track and intensity in Figure 5. Prithvi WxC demon-\\nstrated superior accuracy in both track and intensity predictions. The mean track error for Prithvi\\nWxC was 63.9 km compared to the observed tracks, significantly outperforming the MERRA-2\\ntrained FourCastNet (201.939 km) and the ERA5 trained FourCastNet (262.323 km). Moreover,\\nthe Prithvi WxC accurately forecasted both the time and location of Ida’s landfall, with a landfall\\nlocation error of less than 5 km, in contrast to errors greater than 20 km for the other models. Inten-\\nsity predictions, measured in MSLP and 10-meter sustained wind speed, also favored Prithvi WxC,\\nwhich outperformed the MERRA-2 trained FourCastNet and showed reasonable consistency with\\nthe ERA5 trained FourCastNet. Spatial distribution of Sea Level Pressure (SLP) for a 60-hour fore-\\ncast (valid for 12 UTC on 2021-08-29) are shown the figure 5 c-e. Among the models, the WxC\\nmodel predicts the hurricane landfall most accurately in terms of both spatial location and timing,\\ncompared to the HURDAT reference.\\nFigure 5: (a) The track of Category 4 Hurricane Ida (2021) is shown from HURDAT, MERRA-\\nFCN (FourCastNet model trained on MERRA-2 dataset), ERA-FCN (FourCastNet model trained\\non ERA5 dataset), and WxC models. All models were initialized at 00 UTC on 2021-08-27. The\\ntrack errors of the models, compared to the HURDAT track, are 201.9 km for MERRA-FCN, 262.32\\nkm for ERA-FCN, and 63.9 km for WxC, as noted in the legend. (b) A 5-day forecast of Mean Sea\\nLevel Pressure (MSLP) from MERRA-FCN, ERA-FCN, and WxC models. (c-e) Spatial distribution\\nof Sea Level Pressure (SLP) for a 60-hour forecast (valid for 12 UTC on 2021-08-29). Among the\\nmodels, the WxC model predicts the hurricane landfall most accurately in terms of both spatial\\nlocation and timing, compared to the HURDAT reference.\\nFor a comprehensive assessment, Figure 6 presents the mean track, MSLP, and windspeed errors\\nover a five-day forecast for all the hurricanes included in the robust evaluation (table 6). Prithvi\\nWxC consistently demonstrated lower track errors compared to both versions of FourCastNet, with\\nthe error gap increasing with longer lead times. By the end of the five-day forecast, the WxC model’s\\n9',\n",
       " 'track error was 200 km less than that of the FourCastNet models. While the WxC model outper-\\nformed the MERRA-2 trained FourCastNet in MSLP and windspeed predictions, it was marginally\\noutperformed by the ERA5 trained FourCastNet, likely due to the finer spatial resolution in the\\nERA5 dataset.\\nFigure 6: 5 days composite (75 difference initial conditions) forecast of track errors, MSLP errors\\nand WS errors from MERRA-FourCastNet, ERA-FourCastNet and WxC models\\n3 P RITHVI WXC: D OWNSTREAM VALIDATION\\nIn what follows we will look at a number of downscaling applications realized via fine-tuning. As we\\nwill see, this is frequently non-trivial. We will see changes of the dataset from MERRA-2 to ERA5\\nor CORDEX, changes in spatial and temporal resolution of the data, changes in selected variables\\nand pressure levels and finally a change of spatial domain. With so much variability using the model\\nas is and simply tuning a new head for each problem will lead to subpar results. Instead, we always\\nadd new embedding and output layers and sometimes select other architecture elements. The general\\npattern is that Prithvi WxC forms the typically frozen core of a model with a few additional layers\\nthat are then trained from scratch.\\n3.1 D OWNSCALING\\nDownscaling models are used to refine low-resolution data to provide localized information. Several\\nstudies (Doury et al., 2023) (Lessig et al., 2023) (Nguyen et al., 2023a) (Stengel et al., 2020) employ\\nAI models as downscaling emulators to learn the relationship between low-resolution input data and\\nhigh-resolution output fields. We use a pretrainedPrithvi WxC to recover the spatial structure of\\ncoarsened near surface temperature for two different datasets - MERRA-2, and CORDEX-CMIP5-\\nRCP8.5 - with different input variables and different input resolutions.\\n3.1.1 D OWNSCALING ARCHITECTURE\\nWe use the architecture 7 to fine-tune Prithvi WxC for the downscaling task. The patch embedding\\nlayer encodes static and dynamic data for surface variables and variables at different pressure levels\\nand optionally for multiple time steps. The first upscaling module is used for shallow feature extrac-\\ntion for lower frequency components and also used to control the token resolution that is input to the\\nPrithviWxC model. This follows a deeper feature extraction by the pretrainedtransformer model.\\nSince we set the masking ratio in the encoder to 0 % and the data becomes dense, we may introduce\\na Swin-shift in the encoder. Note that we can make this change while keeping the core transformer\\nlayers frozen . Following (Liang et al., 2021), we use a convolution layer after the transformer to\\nenhance translational equivariance, which is important in downscaling when using different local\\ngrids. The residual connection between the shallow and deep feature extraction layer allows com-\\nbining lower spatial frequency information with the higher spatial frequency information. The final\\nupscale layer focuses on extracting and refining specified output fields.\\n3.1.2 F INETUNING PRITHVI WXCFOR MERRA-2 DOWNSCALING\\nTo validate the overall downscaling performance in a clean setup that isolates model performance\\nfrom dataset questions, we finetune a 6x weather downscaling model for 2m surface temperature\\n10',\n",
       " 'Prithvi WxC\\nencoder-decoder\\nfrozen\\nEmbedding\\n256 channels\\nUpscale\\nPixel shuffleConv2D\\n128 channelsUpscale\\nPixel shuffleMERRA-2 (6x coarser):\\n300 x 375 kmMERRA-2:\\n50 x 62.5 km\\nCORDEX:\\n12.5 kmCORDEX (12x coarser):\\n150 kmFigure 7: Finetuning Architecture of Prithvi WxC for downscaling\\nTable 1: Performance evaluation of the Prithvi WxC downscaling model compared to baselines of\\nnearest neighbor and bilinear interpolation. Spatial RMSE (K), temporal RMSE (K), and temporal\\ncorrelation is evaluated on MERRA-2 2m air temperature (T2m) for a 1-year period from 2021-01-\\n01 to 2021-30-12 (2,912 samples); and on CORDEX near-surface air temperature (tas) for a 5-year\\nperiod from 2096-01-01 to 2100-12-29 (1,829 samples) on the RCP4.5 scenario.\\nMERRA2 - T2m (K) CORDEX - tas (K)\\nsp. RMSE tp. RMSE tp. corr. sp. RMSE tp. RMSE tp. corr.\\nNearest 3.22 2.46 0.89 1.89 1.14 0.99\\nBilinear 3.08 2.34 0.90 1.47 0.90 1.00\\nPrithvi WxC 0.73 0.64 0.98 0.44 0.37 1.00\\nusing MERRA-2 data. The input data variables are the same as used for pre-training. We first\\ncoarsen MERRA-2 data from dimension 361 x 576 (50km x 62.5km resolution) to dimension 60 x\\n96 (300km x 375km resolution), and secondly apply a smoothing operation in form of a convolution\\nwith a 3x3 pixels kernel. Upscaling by a factor 2 before Prithvi WxC we increase the data resolu-\\ntion to 120 x 192 (150km x 187.5km). By using a patch size of 1 for tokenization, we make the\\ntoken resolution similar to the token resolution that Prithvi WxC model was pretrainedon (100km x\\n125km). We then upscale by a factor of 3 to restore the low-resolution data to the original resolution\\nof the 360 x 576 (50km x 62.5km).\\nFigure 8 visualizes the downscaling performance for a single timestamp. Following the example\\nof the ClimateLearn benchmark (Nguyen et al., 2024), we compare the model performance with\\nnearest neighbor and bilinear interpolation baselines. As the power spectra in Figure 8 (c) show, the\\ninterpolation baselines are poor at reconstructing the higher frequency wavenumbers of the ground\\ntruth, while the fine-tuned Prithvi WxC downscaling model is able to do so. The model performance\\nis further evaluated on the entire validation period between 2021-01-01 and 2021-12-30 and results\\nare summarized in Table 1. Compared to the interpolation baselines, Prithvi WxC improves spatial\\nand temporal RMSE values by over a factor of 4 and also shows the best temporal correlation.\\nAs the ClimateLearn benchmark reports improvements by over a factor of 2 when downscaling 2x\\ncoarsened ERA5 2m temperature, we validate with our 6x MERRA-2 downscaling experiment that\\nwe are able to finetune a high-performing Prithvi WxC downscaling model within a clean setup\\nwhere all input variables are the same as in pre-training.\\n3.1.3 F INETUNING PRITHVI WXCFOR CORDEX DOWNSCALING\\nWe now switch from a global to a regional context as we focus on data from the Coordinated Re-\\ngional Climate Downscaling Experiment (CORDEX). Specifically, we use a subset of data from\\nthe EURO-CORDEX simulations (Jacob et al., 2014) at a resolution of 0.11◦x 0.11◦(12.5 km\\n11',\n",
       " 'Figure 8: Downscaling MERRA-2 2m air temperature (t2m) for a sample timestamp on 2021-01-01\\nat 3 UTC. (a) Visual comparison of input variable t2m, with 6 x coarsening and smoothing; the output\\nof the fine-tuned downscaling model; and the corresponding ground-truth. (b) Residuals between\\nmodel prediction and ground-truth with RMSE of 0.76 K. A bias of 0.02 K indicates a negligible\\nmodel overestimation. (c) Power spectra of t2m input, ground-truth, nearest-neighbor interpolation,\\nbilinear interpolation, and downscaling model. Especially towards higher frequencies, the ground-\\ntruth power spectrum is best fit by the Prithvi WxC downscaling model.\\nx 12.5 km) covering a domain over Europe (EUR-11 CORDEX) and based on the regional cli-\\nmate model CNRM-ALADIN63 (Nabat et al., 2020), which is driven by the global climate model\\nCNRM-CM5 (V oldoire et al., 2013). In contrast to the case of previous section 3.1.2, this changes\\nthe dataset, the temporal step as well as the domain from the pretraining case. i.e. we are now tuning\\nthe model for a local context on a dataset that comprises significantly less inputs than the original\\nMERRA-2 dataset.\\nWe finetune a 12x climate downscaling model for daily mean near-surface air temperature for a\\nperiod from 2006 to 2100 under scenario RCP8.5 (Moss et al., 2010). All CORDEX input data\\nvariables (daily means) are shown in Table 5. Following (Doury et al., 2023), we use a perfect\\nmodel framework, downscaling coarsened regional climate simulations, rather than training a model\\nto map GCM simulations to RCM simulations which may not be very well correlated. First, we\\ncoarsen the input data of dimension 444 x 444 (12.5 km x 12.5 km resolution) to dimension 37 x\\n37 (150 km x 150 km resolution) and apply a smoothing convolution such as for MERRA-2. One\\nupscaling layer before the Prithvi WxC backbone increases the data resolution by a factor of 3 to a\\ndimension of 111 x 111 (50 km x 50 km resolution) and two upscaling layers after the Prithvi WxC\\nbackbone increase resolution by a factor of 2 each to restore the CORDEX data’s original 12.5 km\\nx 12.5 km resolution.\\nModel performance is evaluated on data from simulation scenario RCP4.5 which was not seen dur-\\ning training. Results of a single timestamp are shown in Figure 9. Similar as for the MERRA-2\\ndownscaling, the power spectra in Figure 9 (c) demonstrate better reconstruction of higher frequency\\nwavenumbers by the finetuned Prithvi WxC downscaling model compared to the interpolation base-\\nlines. We evaluated the model performance over a time horizon of 5 years from 2096-01-01 to\\n2100-12-29. The average metrics displayed in Table 1 indicate improvements of spatial and tem-\\nporal RMSE values by factors of around 3. Temporal correlation values are generally high across\\nall methods which is most likely explained by the fact that we are downscaling daily mean values\\nof near-surface air temperature. In their perfect model world experiment, Doury et al. (2023) re-\\nport a mean spatial RMSE of 0.55 K when including the target variable (tas) as input feature. Our\\nmean spatial RMSE is 0.44 K, calculated on a bigger spatial domain (corresponding approximately\\nto the EURO-CORDEX simulation domain) without masking the sea and over a shorter time period\\n12',\n",
       " 'Figure 9: Downscaling CORDEX near-surface air temperature (tas) for a sample timestamp on\\n2099-01-01 at 12 pm. (a) Visual comparison of input variable tas, with 12 x coarsening and smooth-\\ning; the output of the fine-tuned downscaling model; and the corresponding ground-truth. (b) Resid-\\nuals between model prediction and ground-truth with RMSE of 0.51 K. A bias 0.06 K indicates a\\nslight model overestimation. (c) Power spectra of tas input, ground-truth, nearest neighbor inter-\\npolation, bilinear interpolation, and downscaling model. Especially towards higher frequencies, the\\nground-truth power spectrum is best fit by the Prithvi WxC downscaling model.\\nof 5-years. Bearing these differences in mind, we are confident that finetuning the Prithvi WxC\\ndownscaling model with frozen MERRA-2 pretrained backbone on a new dataset of distinct na-\\nture resulted in a competitive downscaling model. Future work will encompass downscaling from\\na global climate model to a regional climate model, where the true advantage of AI-based climate\\nemulators can come to light.\\n3.2 C LIMATE MODEL PARAMETERIZATION FOR GRAVITY WAVE FLUX\\nThis task uses the pretrained Prithvi WxC to create a fine-tuned model for climate applications.\\nThe fundamental question being: can we (re-)use large AI models to develop improved, data-driven\\nclimate model parameterizations for small-scale atmosphere-ocean processes?\\nBackground: Atmospheric gravity waves (GWs) are intermittent, small-scale ( O(1) toO(1000)\\nkm) perturbations generated around thunderstorms, jet disturbances, flow over mountains, etc.\\n(Fritts & Alexander, 2003; Achatz et al., 2023). Gravity waves couple the different layers of the\\natmosphere by carrying surface momentum to stratospheric and even mesospheric heights. Yet,\\nmost climate models fail to resolve them owing to limited resolution. Thus, they belong to a class\\nof key physical processes crucial to the earth’s momentum budget but only crudely represented in\\ncoarse-climate models using inadequate physical parameterizations .\\nAn improved parametric representation of gravity waves in comprehensive climate models can po-\\ntentially improve the representation of the seasonal transitions (McLandress et al., 2012), clear air\\nturbulence (Plougonven & Zhang, 2014), Antarctic extreme heat (Choi et al., 2024), and tropical\\npredictability (Baldwin et al., 2001); leading to more certain climate predictions and advancements\\nin mechanistic understanding.\\nFrom an AI perspective, this downstream prediction task moves from predicting the large-scale\\natmospheric state prediction to smaller-scale state prediction, and leverages the cross-scale learn-\\ning from pre-training. As such, the finetuning task is defined to use the latent space of Prithvi\\nto develop data-driven physical parameterizations to provide missing sub-grid scale variability in\\ncoarse-climate models at zero-lag. This is somewhat akin to the downscaling task where CORDEX\\n13',\n",
       " 'is used to augment missing sub-grid information during fine-tuning. For this task, the model is fine-\\ntuned using high-fidelity, high-resolution gravity wave data extracted from ERA5 (which resolves a\\nsubstantial portion of the atmospheric gravity waves, if not all).\\n3.2.1 E XTRACTING GW DATA FOR FINETUNING .\\nThe goal is to accurately predict the momentum fluxes carried by waves generated in different parts\\nof the globe by different processes, given the background atmospheric state. The approach is similar\\nto that followed by traditional single-column parameterizations (Lott & Miller, 1997; Scinocca,\\n2003; Kim et al., 2003). Here, we do so by learning from high-resolution data. In very simple\\nterms, given the background atmospheric state around a mountain (e.g., Andes), or around tropical\\nstorm, can our ML model predict whether the waves are spontaneously generated, and if they are,\\ncalculate the net momentum fluxes they carry; not unlike predicting the cloud cover for a given set\\nof atmospheric conditions.\\nWe use four years of ERA5 global reanalysis on 137 model vertical levels and 30 km horizontal\\nresolution at hourly-frequency to prepare the training data for fine-tuning. The top 15 levels, i.e.,\\nlevels above 45 km are removed due to artifical sponge damping in effect, so effectively 122 vertical\\nlevels. The model takes the zonal wind speed ( u), meridional wind speed ( v), the temperature\\n(T), and pressure ( p), along with positional variables latitude, longitude, and surface height as input.\\nThese variables collectively describe the background state of the atmosphere. The model outputs the\\ndirectional momentum fluxes carried by gravity waves. These fluxes describe the net instantaneous\\nmomentum the gravity waves carry. These directional fluxes are mathematically expressed as the\\ncovariances (u′ω′, v′ω′), and are computed using Helmholtz decomposition using the horizontal\\n(u,v)=(U,V) and vertical wind speeds ( ω=OMEGA). Both the input and output are conservatively\\ncoarse-grained to a 64 ×128 (≈300 km) latitude-longitude grid to be consistent with a typical coarse-\\nclimate model and to remove phase dependencies of the calculated fluxes.\\n[B 488 64 128]\\nPrithvi WxC\\nencoder-decoder\\nfrozen\\nu,v,t,p\\n[B 488 64 128]\\nu/uni2032  /uni03C9/uni2032  ,v/uni2032  /uni03C9/uni2032  ,/uni03B8\\nEncoder block 1\\n160 channels\\nEncoder block 2\\n320 channels120 km skip connection\\nEncoder block 3\\n640 channels\\nEncoder block 4\\n1280 channelsDecoder block 1\\n1280 channelsDecoder block 2\\n640 channelsDecoder block 3\\n320 channelsDecoder block 4\\n160 channels\\nFigure 10: Finetuning Architecture of Prithvi WxC for parameterization of gravity wave flux\\n3.2.2 F INETUNING PRITHVI WXC\\nThe architecture schematic for the finetuning is shown in Figure 10. During fine-tuning Prithvi WxC,\\nwe freeze the encoder and decoder part of the model. The frozen encoder is preceded by 4 learnable\\nconvolution blocks each with an increasing number of hidden channels, i.e., C, 2C, 4Cand then\\n8C, where C= 160. Likewise, the frozen decoder is succeeded by 4 new learnable convolution\\nblocks. Since gravity wave flux prediction is an instantaneous flux calculation task, we fix the lead\\ntimeδtto zero. The instantanous model input for fine-tuning has shape [1, 488, 64, 128] where the\\n488 channels comprise the four background variables u,v,tandpon 122 vertical levels each, and\\non a 64 ×128 horizontal grid, as discussed above. The model was fine-tuned to produce an output\\n14',\n",
       " \"with shape [366, 64, 128] comprising of the potential temperature, u′ω′, and v′ω′on 122 vertical\\nlevels each.\\nThe fine-tuning model leveraged a U-Net like architecture to allow the model to extract high-\\nfrequency information from the given data source. We re-emphasize that Prithvi WxC was pre-\\ntrained on the MERRA-2 dataset but for fine-tuning we are using the downscaled ERA5 dataset.\\nMore importantly, the finetuned model uses global information as input to predict global fluxes as\\noutput, providing a direct contrast to traditional single-column parameterizations. Access to global\\ninformation allows the model to learn the horizontal propagation of gravity waves.\\n3.2.3 R ESULTS AND DESCRIPTION\\nu′′\\n(a) resolved, ERA5 (b) Fine-tuned model\\nv′′\\n(c) resolved, ERA5 (d) Fine-tuned model1.0\\n0.5\\n0.00.51.0\\nzonal flux u'′\\n1.0\\n0.5\\n0.00.51.0\\nmerid. flux v'′\\nmomentum flux prediction @ 12 km height\\nFigure 11: True vs. predicted (non-dimensionalized) momentum fluxes in the upper troposphere (12\\nkm height) for the gravity wave flux parameterization downstream task. (a) and (c) respectively show\\nthe fluxes u′ω′andv′ω′respectively from ERA5, and (b) and (d) show the respective predictions\\nfrom the fine-tuned model. All fluxes are monthly averaged for May 2015. The vertical derivative\\nof the fluxes represents the wind-forcing tendencies due to gravity waves in the atmosphere and can\\nbe used to represent a portion of unresolved sub-grid tendencies in climate models.\\nAs a straightforward test, we look at the climatological distribution, i.e., the monthly-averaged mo-\\nmentum fluxes in the upper troposphere, and compare the spatial distribution of the predicted di-\\nrectional fluxes with the validation data from ERA5 (Figure 11). The prediction from the baseline\\nclosely agrees with the true flux distribution in the upper troposphere. The nature and properties\\nof the waves over land can be significantly different from waves over the ocean. Therefore, get-\\nting a strong agreement over both the ocean and the land indicates effective learning. For instance,\\nenhanced fluxes over the Rocky Mountains, the Andes, and the Himalayas indicates the finetuned\\nmodel skillfully predicts the stationary waves generated over mountain ranges. Likewise, the tropi-\\ncal band of positive flux (in Figure 11b) in the tropics points to effective learning of non-stationary\\ngravity waves generated around intense convective and precipitation systems. In fact, the finetuning\\nmodel even outperforms our task-specific baselines created using MLPs and Attention U-Nets.\\n3.2.4 S CIENTIFIC IMPORTANCE AND BROADER IMPACT\\nWithout loss of generality, the same finetuning procedure can be applied to develop parameteriza-\\ntions for other sub-grid atmospheric processes of relevance to climate; albeit with some tweaks. A\\ncoarse climate model with a typical resolution of O(100) km fails to capture most gravity wave\\neffects (or clouds, or fine-scale turbulence) due to its inability to resolve the smaller-scales. Owing\\nto periodic data assimilation and higher-resolution, numerical weather prediction models are largely\\n15\",\n",
       " 'unaffected by these biases. Running climate models at a high-resolution over multiple centuries,\\nhowever, is computationally not so feasible. To address this, we have proposed one climate-focused\\napplication of Prithvi WxC and demonstrated its effectiveness. This model can subsequently be in-\\ntegrated with coarse-resolution climate models of varying complexity to account for the “missing”\\ngravity wave physics and correct the physics tendencies. The accuracy of the predicted fluxes also\\npoints to the remarkable effectiveness of the fine-tuning process in blending task-specific data from\\nheterogenous sources.\\n4 D ISCUSSION AND CONCLUSIONS\\nThe development of accurate and efficient weather and climate models is crucial for understanding\\nand predicting Earth’s complex atmospheric-oceanic system. While traditional numerical weather\\nprediction (NWP) models have made significant strides, they require substantial computational re-\\nsources. The emergence of deep learning, particularly foundation models pretrained on vast datasets,\\noffers a promising alternative for weather and climate modeling.\\nThis study introduces Prithvi WxC, a 2.3 billion parameter foundation model designed for weather\\nand climate applications. Trained on 160 atmospheric variables from NASA’s Modern-Era Ret-\\nrospective Analysis for Research and Applications, Version 2 (MERRA-2) dataset, Prithvi WxC\\nleverages a scalable and flexible transformer-based architecture to capture both regional and global\\ndependencies in atmospheric data. Unlike task-specific deep learning models, Prithvi WxC aims\\nto address a diverse set of downstream tasks, aligning with the foundation model paradigm preva-\\nlent in AI research. To achieve this, the model introduces a new architecture and novel objective\\nfunction. The latter combines masked reconstruction with forecasting, incorporating climatological\\ninformation to enhance its generalizability.\\nThe zero-shot evaluation introduces reconstruction as a new benchmark, revealing that the model\\nexcels in forecasting at shorter lead times. We hypothesize that this strength stems from the masking\\nobjective, which encourages Prithvi WxC to grasp atmospheric dynamics with limited temporal\\nprogression.\\nWhen it comes to fine-tuning, it’s important to highlight the diversity of datasets, parameters,\\nand resolutions addressed in the downscaling and parameterization examples. In both cases, we\\ndemonstrate that a pretrained, frozen transformer trained on a single dataset can be effectively com-\\nbined with additional architectural components to achieve strong results on new tasks with different\\ndatasets. Furthermore, the CORDEX downscaling case showcases the model’s ability to operate\\nin both global and regional contexts, a characteristic that we attribute to the heavy use of “global”\\nmasking during pretraining.\\nEven though there is no previous work on AI-based downscaling using MERRA-2, we chose this\\nexample to isolate the model’s and architecture’s downscaling performance from questions of dis-\\ntribution shift when changing datasets. Here, we found that the fine-tuned Prithvi WxC model\\nimproves by more than a factor of 4 over interpolation baselines. This 6x downscaling compares\\nto an improvement factor of 2 when doing 2x downscaling with ERA5 data in the ClimateLearn\\nbenchmarks. That is, we have doubled the performance for a threefold resolution increase, evidence\\nof strong performance. This is mirrored by the more applicable CORDEX example which compares\\nfavorably to the results of Doury et al. (2023).\\nFinetuning Prithvi WxC also demonstrates that large transformer-based foundation models can ef-\\nfectively learn mesocale atmospheric evolution, helping to streamline, enhance, and accelerate the\\ndevelopment of physical parameterizations in climate models, which in turn improves prediction\\naccuracy on interannual timescales. The fine-tuned model produces significantly improved predic-\\ntions across all six hotspots, including both the relatively smoother fluxes over the Andes, Southern\\nOcean, Newfoundland, and the Scandinavian Mountains, as well as the more turbulent fluxes over\\nthe Pacific Ocean and Southeast Asia. Notably, for the Andes (mountain waves) and the Southern\\nOcean (non-mountain waves), the fine-tuned model achieves correlation coefficients of 0.99 and\\n0.97, respectively, when compared to the observed fluxes.\\nThe latent encoder-decoder space of Prithvi WxC foundation model captures a comprehensive un-\\nderstanding of atmospheric evolution by training on vast amounts of data, including winds, tem-\\nperature, humidity, radiation, and soil moisture. Instead of building task-specific ML-models from\\n16',\n",
       " 'scratch, these pretrained encoders can be used to develop more precise data-driven models of atmo-\\nspheric processes.\\nACKNOWLEDGMENTS\\nWe would like to thank S. Karthik Mukkavilli who contributed in the early stages of this project. We\\nwould also like to thank Shubha Ranjan from NASA Advanced Supercomputing (NAS) Division,\\nand Mike Little from Goddard Spaceflight Center for their help and support, and McKenzie Hicks\\nand Elizabeth Fancher from NASA IMPACT as well as Kathy Duviella from IBM Research for\\nproject management support. Finally, we would like to thank Wei Ji Leong and Raghu Kiran Ganti.\\nV . Anantharaj is supported by the Office of Science of the U.S. Department of Energy under Contract\\nNo. DE-AC05-00OR22725. Aditi Sheshadri and Aman Gupta are supported by Schmidt Sciences,\\nLLC, a philanthropic initiative founded by Eric and Wendy Schmidt, as part of the Virtual Earth\\nSystem Research Institute (VESRI). Aditi Sheshadri also acknowledges support from the National\\nScience Foundation through Grant OAC-2004492.\\nThis work was supported by NASA’s Office of Chief Science Data Officer and Earth Science Di-\\nvision’s Earth Science Scientific Computing, Earth Science Data Systems Program, and the Earth\\nScience Modeling and Analysis Program.\\nDATA AVAILABILITY\\nThe code for the Prithvi WxC model is available at https://github.com/NASA-IMPACT/\\nPrithvi-WxC . The fine-tuning code for climate model parameterization for gravity wave Flux\\nis available at https://github.com/NASA-IMPACT/gravity-wave-finetuning .\\nThe model checkpoints and sample data are available at https://huggingface.co/\\nPrithvi-WxC .\\nREFERENCES\\nUlrich Achatz, M. Joan Alexander, Erich Becker, Hye-Yeong Chun, Andreas D ¨ornbrack, Laura Holt,\\nRiwal Plougonven, Inna Polichtchouk, Kaoru Sato, Aditi Sheshadri, Claudia Christine Stephan,\\nAnnelize van Niekerk, and Corwin J. Wright. Atmospheric Gravity Waves: Processes and Pa-\\nrameterization. Journal of the Atmospheric Sciences , -1(aop), November 2023. ISSN 0022-4928,\\n1520-0469. doi: 10.1175/JAS-D-23-0210.1.\\nMarcin Andrychowicz, Lasse Espeholt, Di Li, Samier Merchant, Alexander Merose, Fred Zyda,\\nShreya Agrawal, and Nal Kalchbrenner. Deep learning for day forecasts from sparse observations.\\narXiv preprint arXiv:2306.06079 , 2023.\\nM. P. Baldwin, L. J. Gray, T. J. Dunkerton, K. Hamilton, P. H. Haynes, W. J. Randel, J. R. Holton,\\nM. J. Alexander, I. Hirota, T. Horinouchi, D. B. A. Jones, J. S. Kinnersley, C. Marquardt, K. Sato,\\nand M. Takahashi. The quasi-biennial oscillation. Reviews of Geophysics , 39(2):179–229, 2001.\\nISSN 1944-9208. doi: 10.1029/1999RG000073.\\nKaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, and Qi Tian. Pangu-weather:\\nA 3d high-resolution model for fast and accurate global weather forecast. arXiv preprint\\narXiv:2211.02556 , 2022.\\nKaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, and Qi Tian. Accurate medium-\\nrange global weather forecasting with 3d neural networks. Nature , 619(7970):533–538, 2023.\\nCristian Bodnar, Wessel P Bruinsma, Ana Lucic, Megan Stanley, Johannes Brandstetter, Patrick\\nGarvan, Maik Riechert, Jonathan Weyn, Haiyu Dong, Anna Vaughan, et al. Aurora: A foundation\\nmodel of the atmosphere. arXiv preprint arXiv:2405.13063 , 2024.\\nChristopher S. Bretherton, Brian Henn, Anna Kwa, Noah D. Brenowitz, Oliver Watt-Meyer, Jeremy\\nMcGibbon, W. Andre Perkins, Spencer K. Clark, and Lucas Harris. Correcting Coarse-Grid\\nWeather and Climate Models by Machine Learning From Global Storm-Resolving Simulations.\\nJournal of Advances in Modeling Earth Systems , 14(2):e2021MS002794, 2022. ISSN 1942-2466.\\ndoi: 10.1029/2021MS002794.\\n17',\n",
       " 'Kang Chen, Tao Han, Junchao Gong, Lei Bai, Fenghua Ling, Jing-Jia Luo, Xi Chen, Leiming\\nMa, Tianning Zhang, Rui Su, et al. Fengwu: Pushing the skillful global medium-range weather\\nforecast beyond 10 days lead. arXiv preprint arXiv:2304.02948 , 2023.\\nHyesun Choi, Hataek Kwon, Seong-Joong Kim, and Baek-Min Kim. Warmer Antarctic summers in\\nrecent decades linked to earlier stratospheric final warming occurrences. Commun Earth Environ ,\\n5(1):1–9, January 2024. ISSN 2662-4435. doi: 10.1038/s43247-024-01221-0.\\nFrances V . Davenport and Noah S. Diffenbaugh. Using Machine Learning to Analyze Physi-\\ncal Causes of Climate Change: A Case Study of U.S. Midwest Extreme Precipitation. Geo-\\nphysical Research Letters , 48(15):e2021GL093787, 2021. ISSN 1944-8007. doi: 10.1029/\\n2021GL093787.\\nNoah S. Diffenbaugh and Elizabeth A. Barnes. Data-driven predictions of the time remaining until\\ncritical global warming thresholds are reached. Proceedings of the National Academy of Sciences ,\\n120(6):e2207183120, February 2023. doi: 10.1073/pnas.2207183120.\\nAntoine Doury, Samuel Somot, Sebastien Gadat, Aur ´elien Ribes, and Lola Corre. Regional climate\\nmodel emulator based on deep learning: Concept and first evaluation of a novel hybrid downscal-\\ning approach. Climate Dynamics , 60(5):1751–1779, 2023.\\nZachary I. Espinosa, Aditi Sheshadri, Gerald R. Cain, Edwin P. Gerber, and Kevin J. DallaSanta.\\nMachine Learning Gravity Wave Parameterization Generalizes to Capture the QBO and Response\\nto Increased CO2. Geophysical Research Letters , 49(8):e2022GL098174, 2022. ISSN 1944-8007.\\ndoi: 10.1029/2022GL098174.\\nVeronika Eyring, William D. Collins, Pierre Gentine, Elizabeth A. Barnes, Marcelo Barreiro, Tom\\nBeucler, Marc Bocquet, Christopher S. Bretherton, Hannah M. Christensen, Katherine Dagon,\\nDavid John Gagne, David Hall, Dorit Hammerling, Stephan Hoyer, Fernando Iglesias-Suarez,\\nIgnacio Lopez-Gomez, Marie C. McGraw, Gerald A. Meehl, Maria J. Molina, Claire Monteleoni,\\nJuliane Mueller, Michael S. Pritchard, David Rolnick, Jakob Runge, Philip Stier, Oliver Watt-\\nMeyer, Katja Weigel, Rose Yu, and Laure Zanna. Pushing the frontiers in climate modelling and\\nanalysis with machine learning. Nat. Clim. Chang. , pp. 1–13, August 2024. ISSN 1758-6798.\\ndoi: 10.1038/s41558-024-02095-y.\\nChristoph Feichtenhofer, Yanghao Li, Kaiming He, et al. Masked autoencoders as spatiotemporal\\nlearners. Advances in neural information processing systems , 35:35946–35958, 2022.\\nDavid C. Fritts and M. Joan Alexander. Gravity wave dynamics and effects in the middle atmo-\\nsphere. Reviews of Geophysics , 41(1), 2003. ISSN 1944-9208. doi: 10.1029/2001RG000106.\\nRonald Gelaro, Will McCarty, Max J Su ´arez, Ricardo Todling, Andrea Molod, Lawrence Takacs,\\nCynthia A Randles, Anton Darmenov, Michael G Bosilovich, Rolf Reichle, et al. The modern-era\\nretrospective analysis for research and applications, version 2 (MERRA-2). Journal of Climate ,\\n30(14):5419–5454, 2017.\\nWilliam Gregory, Mitchell Bushuk, Yongfei Zhang, Alistair Adcroft, and Laure Zanna. Machine\\nLearning for Online Sea Ice Bias Correction Within Global Ice-Ocean Simulations. Geophysical\\nResearch Letters , 51(3):e2023GL106776, 2024. ISSN 1944-8007. doi: 10.1029/2023GL106776.\\nAman Gupta, Aditi Sheshadri, Sujit Roy, Vishal Gaur, Manil Maskey, and Rahul Ramachan-\\ndran. Machine learning global simulation of nonlocal gravity wave propagation. arXiv preprint\\narXiv:2406.14775 , 2024.\\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll ´ar, and Ross Girshick. Masked au-\\ntoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer\\nvision and pattern recognition , pp. 16000–16009, 2022.\\nBrian Henn, Yakelyn R. Jauregui, Spencer K. Clark, Noah D. Brenowitz, Jeremy McGibbon, Oliver\\nWatt-Meyer, Andrew G. Pauling, and Christopher S. Bretherton. A Machine Learning Param-\\neterization of Clouds in a Coarse-Resolution Climate Model for Unbiased Radiation. Journal\\nof Advances in Modeling Earth Systems , 16(3):e2023MS003949, 2024. ISSN 1942-2466. doi:\\n10.1029/2023MS003949.\\n18',\n",
       " 'Syed Zahid Husain, Leo Separovic, Jean-Franc ¸ois Caron, Rabah Aider, Mark Buehner, St ´ephane\\nChamberland, Ervig Lapalme, Ron McTaggart-Cowan, Christopher Subich, Paul Vaillancourt,\\net al. Leveraging data-driven weather models for improving numerical weather prediction skill\\nthrough large-scale spectral nudging. arXiv preprint arXiv:2407.06100 , 2024.\\nDaniela Jacob, Juliane Petersen, Bastian Eggert, Antoinette Alias, Ole Bøssing Christensen, Lau-\\nrens M. Bouwer, Alain Braun, Augustin Colette, Michel D ´equ´e, Goran Georgievski, Elena Geor-\\ngopoulou, Andreas Gobiet, Laurent Menut, Grigory Nikulin, Andreas Haensler, Nils Hempel-\\nmann, Colin Jones, Klaus Keuler, Sari Kovats, Nico Kr ¨oner, Sven Kotlarski, Arne Kriegsmann,\\nEric Martin, Erik van Meijgaard, Christopher Moseley, Susanne Pfeifer, Swantje Preuschmann,\\nChristine Radermacher, Kai Radtke, Diana Rechid, Mark Rounsevell, Patrick Samuelsson,\\nSamuel Somot, Jean-Francois Soussana, Claas Teichmann, Riccardo Valentini, Robert Vautard,\\nBj¨orn Weber, and Pascal Yiou. EURO-CORDEX: new high-resolution climate change projec-\\ntions for European impact research. Regional Environmental Change , 14(2):563–578, April 2014.\\nISSN 1436-378X. doi: 10.1007/s10113-013-0499-2. URL https://doi.org/10.1007/\\ns10113-013-0499-2 .\\nMartin Janou ˇsek. Era-interim daily climatology. https://confluence.ecmwf.int/\\ndownload/attachments/24316422/daily_climatology_description.pdf ,\\nJanuary 2011.\\nYoung-Joon Kim, S. D. Eckermann, and Hye-Yeong Chun. An overview of the past, present and\\nfuture of gravity-wave drag parametrization for numerical climate and weather prediction models.\\nAtmosphere-Ocean , 41:1:65–98, 2003. doi: 10.3137/ao.410105.\\nDmitrii Kochkov, Janni Yuval, Ian Langmore, Peter Norgaard, Jamie Smith, Griffin Mooers, Milan\\nKl¨ower, James Lottes, Stephan Rasp, Peter D ¨uben, et al. Neural general circulation models for\\nweather and climate. Nature , pp. 1–7, 2024.\\nNikolay Koldunov, Thomas Rackow, Christian Lessig, Sergey Danilov, Suvarchal K Cheedela,\\nDmitry Sidorenko, Irina Sandu, and Thomas Jung. Emerging ai-based weather prediction models\\nas downscaling tools. arXiv preprint arXiv:2406.17977 , 2024.\\nRemi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato, Fer-\\nran Alet, Suman Ravuri, Timo Ewalds, Zach Eaton-Rosen, Weihua Hu, et al. Graphcast: Learning\\nskillful medium-range global weather forecasting. arXiv preprint arXiv:2212.12794 , 2022.\\nRemi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato, Fer-\\nran Alet, Suman Ravuri, Timo Ewalds, Zach Eaton-Rosen, Weihua Hu, et al. Learning skillful\\nmedium-range global weather forecasting. Science , 382(6677):1416–1421, 2023.\\nSimon Lang, Mihai Alexe, Matthew Chantry, Jesper Dramsch, Florian Pinault, Baudouin Raoult,\\nMariana CA Clare, Christian Lessig, Michael Maier-Gerber, Linus Magnusson, et al. Aifs-\\necmwf’s data-driven forecasting system. arXiv preprint arXiv:2406.01465 , 2024.\\nChristian Lessig, Ilaria Luise, Bing Gong, Michael Langguth, Scarlet Stadler, and Martin Schultz.\\nAtmoRep: A stochastic model of atmosphere dynamics using large scale representation learning.\\narXiv preprint arXiv:2308.13280 , 2023.\\nJingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. Swinir:\\nImage restoration using swin transformer. In Proceedings of the IEEE/CVF international confer-\\nence on computer vision , pp. 1833–1844, 2021.\\nFranc ¸ois Lott and Martin J. Miller. A new subgrid-scale orographic drag parametrization: Its for-\\nmulation and testing. Quarterly Journal of the Royal Meteorological Society , 123(537):101–127,\\n1997. ISSN 1477-870X. doi: 10.1002/qj.49712353704.\\nLaura A. Mansfield, Aman Gupta, Adam C. Burnett, Brian Green, Catherine Wilka, and Aditi She-\\nshadri. Updates on Model Hierarchies for Understanding and Simulating the Climate System: A\\nFocus on Data-Informed Methods and Climate Change Impacts. Journal of Advances in Modeling\\nEarth Systems , 15(10):e2023MS003715, 2023. ISSN 1942-2466. doi: 10.1029/2023MS003715.\\n19',\n",
       " 'Morteza Mardani, Noah Brenowitz, Yair Cohen, Jaideep Pathak, Chieh-Yu Chen, Cheng-Chin Liu,\\nArash Vahdat, Karthik Kashinath, Jan Kautz, and Mike Pritchard. Residual diffusion modeling\\nfor km-scale atmospheric downscaling. 2024.\\nCharles McLandress, John F. Scinocca, Theodore G. Shepherd, M. Catherine Reader, and Gloria L.\\nManney. Dynamical Control of the Mesosphere by Orographic and Nonorographic Gravity Wave\\nDrag during the Extended Northern Winters of 2006 and 2009. J. Atmos. Sci. , 70(7):2152–2169,\\nDecember 2012. ISSN 0022-4928. doi: 10.1175/JAS-D-12-0297.1.\\nAnthony McNally, Christian Lessig, Peter Lean, Eulalie Boucher, Mihai Alexe, Ewan Pinnington,\\nMatthew Chantry, Simon Lang, Chris Burrows, Marcin Chrust, et al. Data driven weather fore-\\ncasts trained and initialised directly from observations. arXiv preprint arXiv:2407.15586 , 2024.\\nRichard H Moss, Jae A Edmonds, Kathy A Hibbard, Martin R Manning, Steven K Rose, Detlef P\\nVan Vuuren, Timothy R Carter, Seita Emori, Mikiko Kainuma, Tom Kram, et al. The next gen-\\neration of scenarios for climate change research and assessment. Nature , 463(7282):747–756,\\n2010.\\nS Karthik Mukkavilli, Daniel Salles Civitarese, Johannes Schmude, Johannes Jakubik, Anne Jones,\\nNam Nguyen, Christopher Phillips, Sujit Roy, Shraddha Singh, Campbell Watson, et al. Ai foun-\\ndation models for weather and climate: Applications, design, and implementation. arXiv preprint\\narXiv:2309.10808 , 2023.\\nPierre Nabat, Samuel Somot, Christophe Cassou, Marc Mallet, Martine Michou, Dominique\\nBouniol, Bertrand Decharme, Thomas Drug ´e, Romain Roehrig, and David Saint-Martin. Modu-\\nlation of radiative aerosols effects by atmospheric circulation over the euro-mediterranean region.\\nAtmospheric Chemistry and Physics , 20(14):8315–8349, 2020.\\nTung Nguyen, Johannes Brandstetter, Ashish Kapoor, Jayesh K Gupta, and Aditya Grover. Climax:\\nA foundation model for weather and climate. arXiv preprint arXiv:2301.10343 , 2023a.\\nTung Nguyen, Rohan Shah, Hritik Bansal, Troy Arcomano, Sandeep Madireddy, Romit Maulik,\\nVeerabhadra Kotamarthi, Ian Foster, and Aditya Grover. Scaling transformer neural networks for\\nskillful and reliable medium-range weather forecasting. arXiv preprint arXiv:2312.03876 , 2023b.\\nTung Nguyen, Jason Jewik, Hritik Bansal, Prakhar Sharma, and Aditya Grover. Climatelearn:\\nBenchmarking machine learning for weather and climate modeling. Advances in Neural Infor-\\nmation Processing Systems , 36, 2024.\\nJaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh Chattopadhyay,\\nMorteza Mardani, Thorsten Kurth, David Hall, Zongyi Li, Kamyar Azizzadenesheli, et al. Four-\\ncastnet: A global data-driven high-resolution weather model using adaptive fourier neural opera-\\ntors. arXiv preprint arXiv:2202.11214 , 2022.\\nRiwal Plougonven and Fuqing Zhang. Internal gravity waves from atmospheric jets and fronts.\\nReviews of Geophysics , 52(1):33–76, 2014. ISSN 1944-9208. doi: 10.1002/2012RG000419.\\nStephan Rasp, Michael S. Pritchard, and Pierre Gentine. Deep learning to represent subgrid pro-\\ncesses in climate models. Proc. Natl. Acad. Sci. U.S.A. , 115(39):9684–9689, September 2018.\\nISSN 0027-8424, 1091-6490. doi: 10.1073/pnas.1810286115.\\nStephan Rasp, Stephan Hoyer, Alexander Merose, Ian Langmore, Peter Battaglia, Tyler Russell,\\nAlvaro Sanchez-Gonzalez, Vivian Yang, Rob Carver, Shreya Agrawal, et al. Weatherbench 2: A\\nbenchmark for the next generation of data-driven global weather models. Journal of Advances in\\nModeling Earth Systems , 16(6):e2023MS004019, 2024.\\nMichele M Rienecker, Max J Suarez, Ronald Gelaro, Ricardo Todling, Julio Bacmeister, Emily\\nLiu, Michael G Bosilovich, Siegfried D Schubert, Lawrence Takacs, Gi-Kong Kim, et al. Merra:\\nNasa’s modern-era retrospective analysis for research and applications. Journal of climate , 24\\n(14):3624–3648, 2011.\\nSujit Roy, Rajat Shinde, Christopher E Phillips, Ankur Kumar, Wei Ji Leong, Manil Maskey, and\\nRahul Ramachandran. Clifford neural operators on atmospheric data influenced partial differential\\nequations. In 12th International Conference on Learning Representations , 2024.\\n20',\n",
       " 'Chaitanya Ryali, Yuan-Ting Hu, Daniel Bolya, Chen Wei, Haoqi Fan, Po-Yao Huang, Vaibhav\\nAggarwal, Arkabandhu Chowdhury, Omid Poursaeed, Judy Hoffman, et al. Hiera: A hierarchi-\\ncal vision transformer without the bells-and-whistles. In International Conference on Machine\\nLearning , pp. 29441–29454. PMLR, 2023.\\nJohannes Schmude and Juan Nathaniel. Efficient representation learning for weather and climate\\ndata. In AGU Fall Meeting Abstracts , volume 2023, pp. GC22C–04, 2023.\\nJohn F. Scinocca. An Accurate Spectral Nonorographic Gravity Wave Drag Parameterization for\\nGeneral Circulation Models. Journal of Atmospheric Sciences , 60(4):667–682, February 2003.\\nISSN 0022-4928, 1520-0469. doi: 10.1175/1520-0469(2003)060 ⟨0667:AASNGW ⟩2.0.CO;2.\\nKaren Stengel, Andrew Glaws, Dylan Hettinger, and Ryan N King. Adversarial super-resolution of\\nclimatological wind and solar data. Proceedings of the National Academy of Sciences , 117(29):\\n16805–16815, 2020.\\nZhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, and Yinxiao\\nLi. Maxvit: Multi-axis vision transformer. In European conference on computer vision , pp. 459–\\n479. Springer, 2022.\\nThomas J Vandal, Kate Duffy, Daniel McDuff, Yoni Nachmany, and Chris Hartshorn. Global\\natmospheric data assimilation with multi-modal masked autoencoders. arXiv preprint\\narXiv:2407.11696 , 2024.\\nAurore V oldoire, Emilia Sanchez-Gomez, Dea Salas y M ´elia, B Decharme, Christophe Cassou,\\nSt´ephane S ´en´esi, Sophie Valcke, Isabelle Beau, A Alias, Matthieu Chevallier, et al. The cnrm-\\ncm5. 1 global climate model: description and basic evaluation. Climate dynamics , 40:2091–2121,\\n2013.\\nJonathan A. Weyn, Dale R. Durran, Rich Caruana, and Nathaniel Cresswell-Clay. Sub-Seasonal\\nForecasting With a Large Ensemble of Deep-Learning Weather Prediction Models. Journal of\\nAdvances in Modeling Earth Systems , 13(7):e2021MS002502, 2021. ISSN 1942-2466. doi:\\n10.1029/2021MS002502.\\nJanni Yuval and Paul A. O’Gorman. Neural-Network Parameterization of Subgrid Momen-\\ntum Transport in the Atmosphere. Journal of Advances in Modeling Earth Systems , 15(4):\\ne2023MS003606, 2023. ISSN 1942-2466. doi: 10.1029/2023MS003606.\\nWen Li Zhao, Pierre Gentine, Markus Reichstein, Yao Zhang, Sha Zhou, Yeqiang Wen, Changjie\\nLin, Xi Li, and Guo Yu Qiu. Physics-Constrained Machine Learning of Evapotranspira-\\ntion. Geophysical Research Letters , 46(24):14496–14507, 2019. ISSN 1944-8007. doi:\\n10.1029/2019GL085291.\\nA T ABLES\\n21',\n",
       " 'Table 2: List of Surface Variables\\nVariable Collection Description\\nU10 M2I1NXASM 10 m zonal wind\\nV10 M2I1NXASM 10 m meridional wind\\nT2M M2I1NXASM 2 m surface temperature\\nQV2M M2I1NXASM 2 m specific humidity\\nPS M2I1NXASM Surface Pressure\\nSLP M2I1NXASM Sea Level Pressure\\nTS M2I1NXASM Skin Temperature\\nTQI M2I1NXASM Column-total ice\\nTQL M2I1NXASM Column-total liquid water\\nTQV M2I1NXASM Column-total watre vapor\\nGWETROOT M2T1NXLND Rootzone soil wetness relative to soil holding capacity\\nLAI M2T1NXLND Leaf area index\\nEFLUX M2T1NXFLX Surface latent heat flux\\nHFLUX M2T1NXFLX Surface sensible heat flux\\nZ0M M2T1NXFLX Surface roughness\\nLWGEM M2T1NXRAD Longwave radiation emitted by the surface\\nLWGAB M2T1NXRAD Longwave radiation absorbed by the surface\\nLWTUP M2T1NXRAD Upward longwave at the top of atmosphere\\nSWGNT M2T1NXRAD Net downward shortwave radiation at the surface\\nSWTNT M2T1NXRAD Net shortwave at top of atmosphere\\nTable 3: List of Native Vertical Level Variables\\nVariable Collection Description\\nU M2I3NV ASM Wind speed/direction\\nV M2I3NV ASM Wind speed/direction\\nOMEGA M2I3NV ASM Vertical motions\\nT M2I3NV ASM Air temperature\\nQV M2I3NV ASM Specific humidity\\nPL M2I3NV ASM Actual mid-level pressure\\nH M2I3NV ASM Mid-layer height (equivalent to the geopotential height)\\nCLOUD M2I3NV ASM Cloud fraction at this layer for radiation\\nQI M2I3NV ASM Cloud mass fraction that is ice\\nQL M2I3NV ASM Cloud mass fraction that is water\\nNominal Pressure (hPa)\\n985|970|925|850|700|600|525|412|288|245|208|150|109|48\\nTable 4: List of Static Variables\\nVariable Dataset Description\\nPHIS M2C0NXASM Surface geopotential height\\nFRLAND M2C0NXASM Fraction of surface that is land\\nFROCEAN M2CONXCTM Fraction of surface that is ocean\\nFRACI M2CONXCTM Fraction of surface that is ice\\n22',\n",
       " 'Table 5: List of CORDEX variables. Experiments use daily mean values of scenario simulations\\nRCP4.5 and RCP8.5 between 2006 and 2100.\\nVariable Level (hPa) Unit Description\\nhus500, hus700, hus850 500, 700, 850 - Specific Humidity\\nta500, ta700, ta850 500, 700, 850 K Air Temperature\\nua500, ua700, ua850 500, 700, 850 m/s Eastward Wind\\nva500, va700, va850 500, 700, 850 m/s Northward Wind\\nzg500, zg700, zg850 500, 700, 850 m Geopotential Height\\npsl surface Pa Sea Level Pressure\\ntas surface K Near-Surface Air Temperature\\nuas surface m/s Eastward Near-Surface Wind\\nvas surface m/s Northward Near-Surface Wind\\nTable 6: List of Hurricanes for Evaluation\\nName (YYYY) Category #IC Initial Conditions\\nJose (2017) C4 4 2017090900, 2017091000, 2017091100,\\n2017091200\\nHarvey (2017) C4 2 2017082400, 2017082500\\nIrma (2017) C5 3 2017090500, 2017090600, 2017090700\\nMichael (2018) C5 2 2018100800, 2018100900\\nFlorence (2018) C4 4 2018091000, 2018091100, 2018091200,\\n2018091300\\nDorian (2019) C5 4 2019083100, 2019090100, 2019090200,\\n2019090300\\nLorenzo (2019) C5 4 2019092500, 2019092600, 2019092700,\\n2019092800\\nHumberto (2019) C3 2 2019091400, 2019091500\\nDelta (2020) C4 2 2020100600, 2020100700\\nLaura (2020) C4 2 2020082300, 2020082400\\nIota (2020) C4 2 2020111400, 2020111500\\nZeta (2020) C3 1 2020102500\\nEta (2020) C4 2 2020110700, 2020110800\\nTeddy (2020) C4 5 2020091400, 2020091500, 2020091600,\\n2020091700, 2020091800\\nIda (2021) C4 3 2021082700, 2021082800, 2021082900\\nGrace (2021) C3 2 2021081700, 2021081800\\nLarry (2021) C3 5 2021090200, 2021090300, 2021090400,\\n2021090500, 2021090600\\nSam (2021) C4 5 2021092500, 2021092600, 2021092700,\\n2021092800, 2021092900\\nIan (2022) C5 4 2022092500, 2022092600, 2022092700,\\n2022092800\\nFiona (2022) C4 4 2022091600, 2022091700, 2022091800,\\n2022091900\\nFranklin (2023) C4 1 2023082200\\nLee (2023) C5 8 2023090500, 2023090600, 2023090700,\\n2023090800, 2023090900, 2023091000,\\n2023091100, 2023091200\\nIdalia (2023) C4 4 2023082700, 2023082800, 2023082900,\\n2023083000\\n23']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_path = \"/Users/aleixlahoz/Desktop/2409.13598v1.pdf\"\n",
    "documents = loader.load_data(document_path)\n",
    "list_pdf_documents = [document.text for document in documents]\n",
    "\n",
    "list_pdf_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/graphragenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sep 30, 15:26:58] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/graphragenv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/graphragenv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/graphragenv/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- WARNING! You are using PLAID with an experimental replacement for FAISS for greater compatibility ----\n",
      "This is a behaviour change from RAGatouille 0.8.0 onwards.\n",
      "This works fine for most users and smallish datasets, but can be considerably slower than FAISS and could cause worse results in some situations.\n",
      "If you're confident with FAISS working on your machine, pass use_faiss=True to revert to the FAISS-using behaviour.\n",
      "--------------------\n",
      "\n",
      "\n",
      "[Sep 30, 15:28:24] #> Creating directory .ragatouille/colbert/indexes/weather_and_climate_paper \n",
      "\n",
      "\n",
      "[Sep 30, 15:28:26] [0] \t\t #> Encoding 111 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]/opt/anaconda3/envs/graphragenv/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      " 25%|██▌       | 1/4 [00:10<00:30, 10.07s/it]/opt/anaconda3/envs/graphragenv/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "100%|██████████| 4/4 [00:33<00:00,  8.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sep 30, 15:28:59] [0] \t\t avg_doclen_est = 176.6666717529297 \t len(local_sample) = 111\n",
      "[Sep 30, 15:28:59] [0] \t\t Creating 2,048 partitions.\n",
      "[Sep 30, 15:28:59] [0] \t\t *Estimated* 19,610 embeddings.\n",
      "[Sep 30, 15:28:59] [0] \t\t #> Saving the indexing plan to .ragatouille/colbert/indexes/weather_and_climate_paper/plan.json ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: number of training points (18630) is less than the minimum recommended (20480)\n",
      "used 20 iterations (4.9246s) to cluster 18630 items into 2048 clusters\n",
      "[0.036, 0.034, 0.038, 0.032, 0.033, 0.035, 0.036, 0.033, 0.033, 0.035, 0.034, 0.033, 0.033, 0.036, 0.034, 0.036, 0.031, 0.034, 0.033, 0.034, 0.036, 0.038, 0.031, 0.037, 0.033, 0.034, 0.034, 0.034, 0.036, 0.038, 0.035, 0.037, 0.036, 0.032, 0.033, 0.035, 0.035, 0.034, 0.034, 0.038, 0.035, 0.035, 0.036, 0.034, 0.036, 0.032, 0.034, 0.036, 0.034, 0.033, 0.034, 0.035, 0.034, 0.034, 0.033, 0.035, 0.037, 0.038, 0.037, 0.031, 0.034, 0.035, 0.034, 0.035, 0.037, 0.037, 0.036, 0.035, 0.033, 0.032, 0.036, 0.035, 0.034, 0.034, 0.036, 0.035, 0.037, 0.036, 0.037, 0.039, 0.036, 0.034, 0.036, 0.035, 0.034, 0.033, 0.035, 0.035, 0.034, 0.038, 0.035, 0.038, 0.033, 0.036, 0.035, 0.036, 0.037, 0.035, 0.036, 0.034, 0.037, 0.038, 0.032, 0.035, 0.037, 0.031, 0.034, 0.032, 0.034, 0.033, 0.036, 0.036, 0.036, 0.033, 0.036, 0.032, 0.035, 0.033, 0.034, 0.036, 0.034, 0.034, 0.036, 0.036, 0.035, 0.039, 0.036, 0.034]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sep 30, 15:29:04] [0] \t\t #> Encoding 111 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:32<00:00,  8.01s/it]\n",
      "1it [00:32, 32.21s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 865.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sep 30, 15:29:36] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Sep 30, 15:29:36] #> Building the emb2pid mapping..\n",
      "[Sep 30, 15:29:36] len(emb2pid) = 19610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2048/2048 [00:00<00:00, 79116.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sep 30, 15:29:36] #> Saved optimized IVF to .ragatouille/colbert/indexes/weather_and_climate_paper/ivf.pid.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done indexing!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'.ragatouille/colbert/indexes/weather_and_climate_paper'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAG.index(\n",
    "    collection=list_pdf_documents,\n",
    "    index_name=\"weather_and_climate_paper\",\n",
    "    max_document_length=256,\n",
    "    split_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading searcher for index weather_and_climate_paper for the first time... This may take a few seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/graphragenv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sep 30, 15:34:26] #> Loading codec...\n",
      "[Sep 30, 15:34:26] #> Loading IVF...\n",
      "[Sep 30, 15:34:26] Loading segmented_lookup_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/graphragenv/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sep 30, 15:34:35] #> Loading doclens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 2319.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sep 30, 15:34:35] #> Loading codes and residuals...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 353.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sep 30, 15:34:35] Loading filter_pids_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sep 30, 15:34:42] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "Searcher loaded!\n",
      "\n",
      "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "#> Input: . Which examples of variables do they use to predict the weather?, \t\t True, \t\t None\n",
      "#> Output IDs: torch.Size([32]), tensor([  101,     1,  2029,  4973,  1997, 10857,  2079,  2027,  2224,  2000,\n",
      "        16014,  1996,  4633,  1029,   102,   103,   103,   103,   103,   103,\n",
      "          103,   103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
      "          103,   103])\n",
      "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/graphragenv/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "results = RAG.search(query=\"Which examples of variables do they use to predict the weather?\", k=3, index_name=\"weather_and_climate_paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'Campbell Watson1,Manil Maskey7,Tsengdar J Lee8,Juan\\nBernabe Moreno1,Rahul Ramachandran7\\n†Equal Contribution,\\n‡Johannes.Schmude@ibm.com , Sujit.Roy@nasa.gov*\\nABSTRACT\\nTriggered by the realization that AI emulators can rival the performance of tra-\\nditional numerical weather prediction models running on HPC systems, there is\\nnow an increasing number of large AI models that address use cases such as fore-\\ncasting, downscaling, or nowcasting. While the parallel developments in the AI\\nliterature focus on foundation models – models that can be effectively tuned to\\naddress multiple, different use cases – the developments on the weather and cli-\\nmate side largely focus on single-use cases with particular emphasis on mid-range\\nforecasting. We close this gap by introducing Prithvi WxC, a 2.3 billion parameter\\nfoundation model developed using 160 variables from the Modern-Era Retrospec-\\ntive Analysis for Research and Applications, Version 2 (MERRA-2).',\n",
       "  'score': 19.805286407470703,\n",
       "  'rank': 1,\n",
       "  'document_id': 'd8f8fd24-b731-45f4-ab86-6edc7e5bc3fe',\n",
       "  'passage_id': 1},\n",
       " {'content': 'Leveraging data-driven weather models for improving numerical weather prediction skill\\nthrough large-scale spectral nudging. arXiv preprint arXiv:2407.06100 , 2024.\\nDaniela Jacob, Juliane Petersen, Bastian Eggert, Antoinette Alias, Ole Bøssing Christensen, Lau-\\nrens M. Bouwer, Alain Braun, Augustin Colette, Michel D ´equ´e, Goran Georgievski, Elena Geor-\\ngopoulou, Andreas Gobiet, Laurent Menut, Grigory Nikulin, Andreas Haensler, Nils Hempel-\\nmann, Colin Jones, Klaus Keuler, Sari Kovats, Nico Kr ¨oner, Sven Kotlarski, Arne Kriegsmann,\\nEric Martin, Erik van Meijgaard, Christopher Moseley, Susanne Pfeifer, Swantje Preuschmann,\\nChristine Radermacher, Kai Radtke, Diana Rechid, Mark Rounsevell, Patrick Samuelsson,\\nSamuel Somot, Jean-Francois Soussana, Claas Teichmann, Riccardo Valentini, Robert Vautard,\\nBj¨orn Weber, and Pascal Yiou.',\n",
       "  'score': 18.530258178710938,\n",
       "  'rank': 2,\n",
       "  'document_id': 'd616f7f1-ef9d-49d9-9915-9fa592b3503d',\n",
       "  'passage_id': 87},\n",
       " {'content': 'ods (Bi et al., 2023; Lam et al., 2023; Mukkavilli et al., 2023). Unlike the traditional physics-based\\napproaches, deep learning models do not directly simulate the underlying physics. Instead, they\\ncapture this through probability distributions derived from model training, a method adapted from\\nnatural language processing and computer vision. This technique has proven surprisingly effec-\\ntive in approximating complex physical systems such as the weather. However, most current deep\\nlearning models for weather are task-specific forecast emulators, which focus solely on the forecast-\\ning problem. (See, however, Koldunov et al. (2024).) Key examples include FourCastNet (Pathak\\net al., 2022), Pangu (Bi et al., 2022), GraphCast (Lam et al., 2022), FengWu (Chen et al., 2023),\\nStormer (Nguyen et al., 2023b) and AIFS (Lang et al., 2024). Machine learning models also show\\npromise for longer-term subseasonal-to-seasonal forecasts (Weyn et al., 2021).',\n",
       "  'score': 18.5067081451416,\n",
       "  'rank': 3,\n",
       "  'document_id': '5d0103e2-0545-41c3-b9d8-fd6cb518adf0',\n",
       "  'passage_id': 4}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------- doc-0 ------------------------------------\n",
      "Campbell Watson1,Manil Maskey7,Tsengdar J Lee8,Juan\n",
      "Bernabe Moreno1,Rahul Ramachandran7\n",
      "†Equal Contribution,\n",
      "‡Johannes.Schmude@ibm.com , Sujit.Roy@nasa.gov*\n",
      "ABSTRACT\n",
      "Triggered by the realization that AI emulators can rival the performance of tra-\n",
      "ditional numerical weather prediction models running on HPC systems, there is\n",
      "now an increasing number of large AI models that address use cases such as fore-\n",
      "casting, downscaling, or nowcasting. While the parallel developments in the AI\n",
      "literature focus on foundation models – models that can be effectively tuned to\n",
      "address multiple, different use cases – the developments on the weather and cli-\n",
      "mate side largely focus on single-use cases with particular emphasis on mid-range\n",
      "forecasting. We close this gap by introducing Prithvi WxC, a 2.3 billion parameter\n",
      "foundation model developed using 160 variables from the Modern-Era Retrospec-\n",
      "tive Analysis for Research and Applications, Version 2 (MERRA-2).\n",
      "---------------------------------- doc-1 ------------------------------------\n",
      "Leveraging data-driven weather models for improving numerical weather prediction skill\n",
      "through large-scale spectral nudging. arXiv preprint arXiv:2407.06100 , 2024.\n",
      "Daniela Jacob, Juliane Petersen, Bastian Eggert, Antoinette Alias, Ole Bøssing Christensen, Lau-\n",
      "rens M. Bouwer, Alain Braun, Augustin Colette, Michel D ´equ´e, Goran Georgievski, Elena Geor-\n",
      "gopoulou, Andreas Gobiet, Laurent Menut, Grigory Nikulin, Andreas Haensler, Nils Hempel-\n",
      "mann, Colin Jones, Klaus Keuler, Sari Kovats, Nico Kr ¨oner, Sven Kotlarski, Arne Kriegsmann,\n",
      "Eric Martin, Erik van Meijgaard, Christopher Moseley, Susanne Pfeifer, Swantje Preuschmann,\n",
      "Christine Radermacher, Kai Radtke, Diana Rechid, Mark Rounsevell, Patrick Samuelsson,\n",
      "Samuel Somot, Jean-Francois Soussana, Claas Teichmann, Riccardo Valentini, Robert Vautard,\n",
      "Bj¨orn Weber, and Pascal Yiou.\n",
      "---------------------------------- doc-2 ------------------------------------\n",
      "ods (Bi et al., 2023; Lam et al., 2023; Mukkavilli et al., 2023). Unlike the traditional physics-based\n",
      "approaches, deep learning models do not directly simulate the underlying physics. Instead, they\n",
      "capture this through probability distributions derived from model training, a method adapted from\n",
      "natural language processing and computer vision. This technique has proven surprisingly effec-\n",
      "tive in approximating complex physical systems such as the weather. However, most current deep\n",
      "learning models for weather are task-specific forecast emulators, which focus solely on the forecast-\n",
      "ing problem. (See, however, Koldunov et al. (2024).) Key examples include FourCastNet (Pathak\n",
      "et al., 2022), Pangu (Bi et al., 2022), GraphCast (Lam et al., 2022), FengWu (Chen et al., 2023),\n",
      "Stormer (Nguyen et al., 2023b) and AIFS (Lang et al., 2024). Machine learning models also show\n",
      "promise for longer-term subseasonal-to-seasonal forecasts (Weyn et al., 2021).\n"
     ]
    }
   ],
   "source": [
    "for i, doc, in enumerate(results):\n",
    "    print(f\"---------------------------------- doc-{i} ------------------------------------\")\n",
    "    print(doc[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With OpenAI Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT=os.getenv(\"AZURE_OPENAI_ENDPOINT\") \n",
    "AZURE_OPENAI_API_KEY=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME_EMBEDDINGS=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME_EMBEDDINGS\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphragenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
