{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/telecombcn-dl/labs-all/blob/master/labs/cnn/lab_cnn_todo.ipynb","timestamp":1633604918354}]},"widgets":{"application/vnd.jupyter.widget-state+json":{"968ae8ca8914492ea0b6aebb517c2b80":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5a2c7aae0b8e4f9bb07ff7aab2a9cdc5","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_56c4fe92a73b47d2ae3fd95a67fd56a1","IPY_MODEL_52b7f1e9669d4ebcbe2d0da0192e5405","IPY_MODEL_3a3dbf800e154ac6a2830678badc1eb7"]}},"5a2c7aae0b8e4f9bb07ff7aab2a9cdc5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"56c4fe92a73b47d2ae3fd95a67fd56a1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c9da94d6852042f8a0f8683c2d84a666","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7361c4707bd6480f9ba2fcf474c767f5"}},"52b7f1e9669d4ebcbe2d0da0192e5405":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_de8249c9718f4c249fb3b7d12cdacf6b","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":9912422,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":9912422,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2a7c8db3557e48198690415573494b91"}},"3a3dbf800e154ac6a2830678badc1eb7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2836f17bc0894688b24af2a4c9ce5c4c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 9913344/? [00:00&lt;00:00, 41208952.36it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8afac572aa4e4ae08444a16307cea37f"}},"c9da94d6852042f8a0f8683c2d84a666":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7361c4707bd6480f9ba2fcf474c767f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"de8249c9718f4c249fb3b7d12cdacf6b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"2a7c8db3557e48198690415573494b91":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2836f17bc0894688b24af2a4c9ce5c4c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8afac572aa4e4ae08444a16307cea37f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"sKQ_CAxyJG8O"},"source":["# Image Classification with a Convolutional Neural Network\n"]},{"cell_type":"markdown","metadata":{"id":"PDTpY5GwJPaF"},"source":["### Advice: Select the GPU Hardware Acceleration in the Runtime environment Menu to train the network fast."]},{"cell_type":"markdown","metadata":{"id":"vYym1OnNLMRq"},"source":["We will start replicating the imports and Dataset from the last lab (where MLPs were used to classify MNIST images)."]},{"cell_type":"code","metadata":{"id":"oJlx5PPIcLq3"},"source":["# MNIST download doesn't work in torchvision<0.9.0\n","# Execute this code to solve it (https://github.com/pytorch/vision/issues/1938#issuecomment-594623431)\n","from six.moves import urllib\n","opener = urllib.request.build_opener()\n","opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n","urllib.request.install_opener(opener)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BvEhbQKQcLq5"},"source":["import numpy as np\n","np.random.seed(1)\n","import torch\n","import torch.optim as optim\n","torch.manual_seed(1)\n","if torch.cuda.is_available():\n","  torch.cuda.manual_seed_all(1)\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms\n","import matplotlib\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","from timeit import default_timer as timer"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-lDIiSkWJbYd"},"source":["### Defining the Hyper-parameters\n","\n","We now define the hyperparameters that are going to be used throughout the notebook\n","to define the network, the data `batch_size`, the training `learning_rate`, and others."]},{"cell_type":"code","metadata":{"id":"zEef2DVdcLq6"},"source":["# Let's define some hyper-parameters\n","hparams = {\n","    'batch_size':64,\n","    'num_epochs':10,\n","    'test_batch_size':64,\n","    'learning_rate':1e-3,\n","    'log_interval':100,\n","}\n","\n","# we select to work on GPU if it is available in the machine, otherwise\n","# will run on CPU\n","hparams['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# whenever we send something to the selected device (X.to(device)) we already use\n","# either CPU or CUDA (GPU). Importantly...\n","# The .to() operation is in-place for nn.Module's, so network.to(device) suffices\n","# The .to() operation is NOT in.place for tensors, so we must assign the result\n","# to some tensor, like: X = X.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DpUxbhqIcLq7","colab":{"base_uri":"https://localhost:8080/","height":477,"referenced_widgets":["968ae8ca8914492ea0b6aebb517c2b80","5a2c7aae0b8e4f9bb07ff7aab2a9cdc5","56c4fe92a73b47d2ae3fd95a67fd56a1","52b7f1e9669d4ebcbe2d0da0192e5405","3a3dbf800e154ac6a2830678badc1eb7","c9da94d6852042f8a0f8683c2d84a666","7361c4707bd6480f9ba2fcf474c767f5","de8249c9718f4c249fb3b7d12cdacf6b","2a7c8db3557e48198690415573494b91","2836f17bc0894688b24af2a4c9ce5c4c","8afac572aa4e4ae08444a16307cea37f","01b9fa687c9d43559f723030434d767d","42e50a5b5b9d4e0782fdb06fe617c307","44f26561e40a4e8ea7a21b9f1dab6166"]},"executionInfo":{"status":"ok","timestamp":1633605281971,"user_tz":-120,"elapsed":2061,"user":{"displayName":"Aleix Lahoz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5UYm5Q8-haFP2moJt24YYH3VUqATN22jr_XBbrw=s64","userId":"17486275261500353019"}},"outputId":"6bdc3a5a-7e5d-4488-eb79-f40070630cd1"},"source":["mnist_trainset = datasets.MNIST('data', train=True, download=True,\n","                                transform=transforms.Compose([\n","                                    transforms.ToTensor(),\n","                                    transforms.Normalize((0.1307,), (0.3081,))\n","                                ]))\n","mnist_testset = datasets.MNIST('data', train=False,\n","                               transform=transforms.Compose([\n","                                   transforms.ToTensor(),\n","                                   transforms.Normalize((0.1307,), (0.3081,))\n","                               ]))\n","\n","train_loader = torch.utils.data.DataLoader(\n","    mnist_trainset,\n","    batch_size=hparams['batch_size'],\n","    shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(\n","    mnist_testset,\n","    batch_size=hparams['test_batch_size'],\n","    shuffle=False)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"968ae8ca8914492ea0b6aebb517c2b80","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/9912422 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"01b9fa687c9d43559f723030434d767d","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/28881 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"42e50a5b5b9d4e0782fdb06fe617c307","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/1648877 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"44f26561e40a4e8ea7a21b9f1dab6166","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/4542 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n","  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"]}]},{"cell_type":"code","metadata":{"id":"IiwQu-kccLq9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633605303404,"user_tz":-120,"elapsed":232,"user":{"displayName":"Aleix Lahoz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5UYm5Q8-haFP2moJt24YYH3VUqATN22jr_XBbrw=s64","userId":"17486275261500353019"}},"outputId":"640766c9-7e46-4d57-8e2f-4f9ad5a12508"},"source":["# We can retrieve a sample from the dataset by simply indexing it\n","img, label = mnist_trainset[0]\n","print('Img shape: ', img.shape)\n","print('Label: ', label)\n","\n","# Similarly, we can sample a BATCH from the dataloader by running over its iterator\n","iter_ = iter(train_loader)\n","bimg, blabel = next(iter_)\n","print('Batch Img shape: ', bimg.shape)\n","print('Batch Label shape: ', blabel.shape)\n","print('The Batched tensors return a collection of {} grayscale images ({} channel, {} height pixels, {} width pixels)'.format(bimg.shape[0],\n","                                                                                                                              bimg.shape[1],\n","                                                                                                                              bimg.shape[2],\n","                                                                                                                              bimg.shape[3]))\n","print('In the case of the labels, we obtain {} batched integers, one per image'.format(blabel.shape[0]))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Img shape:  torch.Size([1, 28, 28])\n","Label:  5\n","Batch Img shape:  torch.Size([64, 1, 28, 28])\n","Batch Label shape:  torch.Size([64])\n","The Batched tensors return a collection of 64 grayscale images (1 channel, 28 height pixels, 28 width pixels)\n","In the case of the labels, we obtain 64 batched integers, one per image\n"]}]},{"cell_type":"markdown","metadata":{"id":"CeyWIoJeLmGA"},"source":["### About Convolutional Neural Networks"]},{"cell_type":"markdown","metadata":{"id":"dfNAe2v5NgYF"},"source":["Previously, we trained a network using fully connected layers to classify images. However, in the lectures we learned that when dealing with images, Convolutional Neural Networks (CNNs) are more convenient because they deal better with local correlations in the data (as with images). So now we are going to train a CNN for multiclass classification.\n","\n","The following lines show what a basic convnet looks like. It's a stack of Conv2D and MaxPooling2D layers.  \n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"5Ho56gJkcLrA"},"source":["# Let's first define a 2D convolutional layer with 1 input channel, 3 output channels and (height=3, width=3) kernel size\n","\n","conv = nn.Conv2d(1, 3, 3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m3sInwgsP46R"},"source":["Importantly, a convnet takes input tensors of shape `(batch, num_channels, image_height, image_width)`. In our case, we will configure our convnet to process inputs of size `(1, H, W)`, which is the format of MNIST images.\n","Let's try with some random image.\n"]},{"cell_type":"code","metadata":{"id":"pOUD2WoDcLrB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633605403599,"user_tz":-120,"elapsed":225,"user":{"displayName":"Aleix Lahoz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5UYm5Q8-haFP2moJt24YYH3VUqATN22jr_XBbrw=s64","userId":"17486275261500353019"}},"outputId":"f4d279a2-ef06-47f6-d0ce-e36d8a67ac6e"},"source":["x = torch.rand(1, 1, 28, 28)\n","y = conv(x)\n","print('Output shape: {} = conv({})'.format(y.shape, x.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output shape: torch.Size([1, 3, 26, 26]) = conv(torch.Size([1, 1, 28, 28]))\n"]}]},{"cell_type":"markdown","metadata":{"id":"aoif3i8gQwA7"},"source":["> Alice: Note that the only actual number we specify from the input data is the number of channels! No image dimensions are given to the convolutional layer. These are just used to shape the data `x`.\n",">\n","> Bob: Why is that?\n",">\n","> <p>Alice: Because of PyTorch magic &#128526;</p>\n",">\n","> <p>Bob: No, seriously. Why? &#128565;</p>\n",">\n","> Alice: Because of the dynamic computational graph (DCG)!\n","\n","**Do you remember all that stuff about a certain dynamic computational graph? Well here it goes in action. Do we need to specify a fixed size for the images as in other frameworks (such as Keras, TensorFlow, etc.) ? Nope. Because we can forward any image size at any time through the same convolutional layer!**\n","\n","As another example. Let's forward an image of size (11, 11)."]},{"cell_type":"code","metadata":{"id":"if-hAWtBcLrC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633605437566,"user_tz":-120,"elapsed":228,"user":{"displayName":"Aleix Lahoz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5UYm5Q8-haFP2moJt24YYH3VUqATN22jr_XBbrw=s64","userId":"17486275261500353019"}},"outputId":"528fc229-33f5-4aa8-cba6-e8726dcdca1d"},"source":["x = torch.rand(1, 1, 11, 11)\n","y = conv(x)\n","print('Output shape: {} = conv({})'.format(y.shape, x.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output shape: torch.Size([1, 3, 9, 9]) = conv(torch.Size([1, 1, 11, 11]))\n"]}]},{"cell_type":"markdown","metadata":{"id":"zTLaDk0rRMWI"},"source":["**Tadáaaa**, it did accept the input data, and give a corresponding output shape. **The only argument related to the data required in the convolutional definition is the number of channels.**\n","\n","### Exercise 1\n","\n","Why do the output spatial dimensions (`dim=2` and `dim=3`) differ from the input ones? Re-define the `conv` layer below setting the appropriate property such that the output spatial shape is the same as the input one. Also configure it to have three output channels. Look at the PyTorch documentation (`https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d`) for more reference."]},{"cell_type":"code","metadata":{"id":"ZSeVkVH3cLrD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633605777367,"user_tz":-120,"elapsed":324,"user":{"displayName":"Aleix Lahoz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5UYm5Q8-haFP2moJt24YYH3VUqATN22jr_XBbrw=s64","userId":"17486275261500353019"}},"outputId":"964b50bb-d0ee-4549-9c91-dc01daf4f935"},"source":["# TODO: Define the conv layer below and ensure that the output tensor shape in\n","# dimensions {H, W} ( as in [1, channels, H, W] ) will be the same as the input in both cases.\n","conv = nn.Conv2d(1, 3, 3, padding='same') #(input channel, output channel, kernel)\n","\n","x = torch.rand(1, 1, 20, 20)\n","y = conv(x)\n","print('Output shape: {} = conv({})'.format(y.shape, x.shape))\n","\n","x = torch.rand(1, 1, 11, 11)\n","y = conv(x)\n","print('Output shape: {} = conv({})'.format(y.shape, x.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output shape: torch.Size([1, 3, 20, 20]) = conv(torch.Size([1, 1, 20, 20]))\n","Output shape: torch.Size([1, 3, 11, 11]) = conv(torch.Size([1, 1, 11, 11]))\n"]}]},{"cell_type":"markdown","metadata":{"id":"kY7hUMauUE9_"},"source":["### About Pooling\n","\n","Pooling refers to a block where downsampling happens. In the case of CNNs, as they process full images throughout a certain stack of layers that can get quite deep, they occupy a lot of memory to store the so called feature maps. Feature maps are the intermediate hidden activations of a CNN. The next image ([from a Quora response](https://www.quora.com/What-is-max-pooling-in-convolutional-neural-networks)) is very self-explainatory of what **Max Pooling** does applied to images and spatial feature maps.\n","\n","![](https://qph.fs.quoracdn.net/main-qimg-40cdeb3b43594f4b1b1b6e2c137e80b7)\n","\n","As you see, it decimates neighboring regions by picking the max value within that region. And that happens for every channel in the feature map (or the image, if it is grayscale/RGB).\n","\n","The are also other pooling methods, like [`AvgPool2d`](https://pytorch.org/docs/stable/nn.html#torch.nn.AvgPool2d), strided Convolutions (simply incrasing the `stride > 1` of the `torch.nn.Conv2d` layer, etc. Each one has its advantages and drawbacks, which are so far out of the scope of this study on how to define a CNN in PyTorch.\n","\n","In any case, a good question for now is: **what are the advantages of using pooling of any kind?**\n","\n","Let's define a small CNN without pooling and another one with pooling, and let's check the amount of memory used by each in terms of feature map usage and the time it takes to forward an image of `512x512` pixels with just `1` input channel (hence greyscale)."]},{"cell_type":"code","metadata":{"id":"jCfaBiFOcLrD"},"source":["NUM_BITS_FLOAT32 = 32\n","\n","# Let's define a class that encapsulates a collection of layers we pass in\n","# for each forwarded layer, it retains the amount of consumed memory for\n","# the returned feature map. It also displays the total amount used after\n","# all blocks are ran.\n","class CNNMemAnalyzer(nn.Module):\n","\n","  def __init__(self, layers):\n","    super().__init__()\n","    self.layers = layers\n","\n","  def forward(self, x):\n","    tot_mbytes = 0\n","    spat_res = []\n","    for layer in self.layers:\n","      h = layer(x)\n","      mem_h_bytes = np.cumprod(h.shape)[-1] * NUM_BITS_FLOAT32 // 8\n","      mem_h_mb = mem_h_bytes / 1e6\n","      print('-' * 30)\n","      print('New feature map of shape: ', h.shape)\n","      print('Mem usage: {} MB'.format(mem_h_mb))\n","      x = h\n","      if isinstance(layer, nn.Conv2d):\n","        # keep track of the current spatial width for conv layers\n","        spat_res.append(h.shape[-1])\n","      tot_mbytes += mem_h_mb\n","    print('=' * 30)\n","    print('Total used memory: {:.2f} MB'.format(tot_mbytes))\n","    return tot_mbytes, spat_res"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bB1Llpg2ZkB7"},"source":["#### Forwarding the 512x512 image through a non-pooled CNN"]},{"cell_type":"code","metadata":{"id":"J6XOYWydcLrE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633605997609,"user_tz":-120,"elapsed":6367,"user":{"displayName":"Aleix Lahoz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5UYm5Q8-haFP2moJt24YYH3VUqATN22jr_XBbrw=s64","userId":"17486275261500353019"}},"outputId":"01b9a68e-d103-4d71-e7e2-157e8eece60a"},"source":["# First, make a plain stack of convlayers\n","cnn = CNNMemAnalyzer(nn.ModuleList([nn.Conv2d(1, 32, 3),\n","                                    nn.Conv2d(32, 64, 3),\n","                                    nn.Conv2d(64, 64, 3),\n","                                    nn.Conv2d(64, 128, 3),\n","                                    nn.Conv2d(128, 512, 3)]))\n","\n","# Let's work with a realistic 512x512 image size\n","# Also, keep track of time to make forward\n","beg_t = timer()\n","nopool_mbytes, nopool_res = cnn(torch.randn(1, 1, 512, 512))\n","end_t = timer()\n","nopool_time = end_t - beg_t\n","print('Total inference time for non-pooled CNN: {:.2f} s'.format(nopool_time))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["------------------------------\n","New feature map of shape:  torch.Size([1, 32, 510, 510])\n","Mem usage: 33.2928 MB\n","------------------------------\n","New feature map of shape:  torch.Size([1, 64, 508, 508])\n","Mem usage: 66.064384 MB\n","------------------------------\n","New feature map of shape:  torch.Size([1, 64, 506, 506])\n","Mem usage: 65.545216 MB\n","------------------------------\n","New feature map of shape:  torch.Size([1, 128, 504, 504])\n","Mem usage: 130.056192 MB\n","------------------------------\n","New feature map of shape:  torch.Size([1, 512, 502, 502])\n","Mem usage: 516.104192 MB\n","==============================\n","Total used memory: 811.06 MB\n","Total inference time for non-pooled CNN: 6.10 s\n"]}]},{"cell_type":"markdown","metadata":{"id":"D4UDx8umZntD"},"source":["#### Forwarding the 512x512 image through a pooled CNN"]},{"cell_type":"code","metadata":{"id":"U3T9FI6XcLrF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633606005020,"user_tz":-120,"elapsed":627,"user":{"displayName":"Aleix Lahoz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5UYm5Q8-haFP2moJt24YYH3VUqATN22jr_XBbrw=s64","userId":"17486275261500353019"}},"outputId":"1a37b62a-3d4f-42bd-abb8-a1b73eacfdd9"},"source":["# Now, let's make a stack of convlayers combined with MaxPoolings\n","cnn = CNNMemAnalyzer(nn.ModuleList([nn.Conv2d(1, 32, 3),\n","                                    nn.MaxPool2d(2),\n","                                    nn.Conv2d(32, 64, 3),\n","                                    nn.MaxPool2d(2),\n","                                    nn.Conv2d(64, 64, 3),\n","                                    nn.MaxPool2d(2),\n","                                    nn.Conv2d(64, 128, 3),\n","                                    nn.MaxPool2d(2),\n","                                    nn.Conv2d(128, 512, 3),\n","                                    nn.MaxPool2d(2)]))\n","\n","beg_t = timer()\n","pool_mbytes, pool_res = cnn(torch.randn(1, 1, 512, 512))\n","end_t = timer()\n","pool_time = end_t - beg_t\n","print('Total inference time for pooled CNN: {:.2f} s'.format(pool_time))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["------------------------------\n","New feature map of shape:  torch.Size([1, 32, 510, 510])\n","Mem usage: 33.2928 MB\n","------------------------------\n","New feature map of shape:  torch.Size([1, 32, 255, 255])\n","Mem usage: 8.3232 MB\n","------------------------------\n","New feature map of shape:  torch.Size([1, 64, 253, 253])\n","Mem usage: 16.386304 MB\n","------------------------------\n","New feature map of shape:  torch.Size([1, 64, 126, 126])\n","Mem usage: 4.064256 MB\n","------------------------------\n","New feature map of shape:  torch.Size([1, 64, 124, 124])\n","Mem usage: 3.936256 MB\n","------------------------------\n","New feature map of shape:  torch.Size([1, 64, 62, 62])\n","Mem usage: 0.984064 MB\n","------------------------------\n","New feature map of shape:  torch.Size([1, 128, 60, 60])\n","Mem usage: 1.8432 MB\n","------------------------------\n","New feature map of shape:  torch.Size([1, 128, 30, 30])\n","Mem usage: 0.4608 MB\n","------------------------------\n","New feature map of shape:  torch.Size([1, 512, 28, 28])\n","Mem usage: 1.605632 MB\n","------------------------------\n","New feature map of shape:  torch.Size([1, 512, 14, 14])\n","Mem usage: 0.401408 MB\n","==============================\n","Total used memory: 71.30 MB\n","Total inference time for pooled CNN: 0.24 s\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"]}]},{"cell_type":"code","metadata":{"id":"6m3NsRehcLrG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633606018281,"user_tz":-120,"elapsed":370,"user":{"displayName":"Aleix Lahoz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5UYm5Q8-haFP2moJt24YYH3VUqATN22jr_XBbrw=s64","userId":"17486275261500353019"}},"outputId":"87878138-d727-4ca1-deba-9d57ab9d140c"},"source":["mem_ratio = 1. - pool_mbytes / nopool_mbytes\n","print('Total saved memory with poolings: ', 100. * mem_ratio)\n","\n","time_ratio = nopool_time / pool_time\n","print('Total inference speed increase with poolings: x{:.1f}'.format(time_ratio))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total saved memory with poolings:  91.20932171879804\n","Total inference speed increase with poolings: x25.6\n"]}]},{"cell_type":"code","metadata":{"id":"8VCxdoI9cLrG","colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"status":"ok","timestamp":1633606028356,"user_tz":-120,"elapsed":661,"user":{"displayName":"Aleix Lahoz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5UYm5Q8-haFP2moJt24YYH3VUqATN22jr_XBbrw=s64","userId":"17486275261500353019"}},"outputId":"b2770310-d001-4633-8baa-dc1cf769636b"},"source":["# Let's plot the width of each feature map as we get deeper into the network\n","_ = plt.plot(nopool_res, label='No pooling')\n","_ = plt.plot(pool_res, label='Pooling')\n","_ = plt.legend()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"OwZhykg5XajA"},"source":["### Key Observations\n","\n","* We save 91.2% of memory having a model which is pooling after the first couple of conv layers.\n","* The model that contains pooling runs 28.5 times faster in inference than the other one.\n","* The width dimension decreases exponentially when inserting the poolings, compared to the one without those poolings.\n","\n","The convolutional operator works by sweeping the kernel filters through the input image. If we pool `x2` in a couple layers, from the 3rd convlayer onwards we have a `x4` smaller spatial resolution. This means it has to run through by far less pixels to process the whole feature map in those layers. Also, each feature occupies much less memory for the reduced resolution. Pooling is hence a practical downsampling to make our nets fit in memory, and also to get the salient features from the previous incoming feature maps (it gets the maximum activation and forwards only that one within a window). In general, for classification, it is usual to have pooling to condense spatial dimensions into less-and-more-abstract ones. This is done by finish processing the last reduced spatial feature map with some fully connected layer that mixes it all up."]},{"cell_type":"markdown","metadata":{"id":"uAN1p6hrcoQn"},"source":["## Grand Finale: Building a (pseudo) LeNet model\n","\n","Here we will gather the puzzle pieces we have so far (tensor manipulations, convs, poolings, fully connected layers, etc.) and we will define a pseudo-LeNet model ([LeNet ref](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)). We say \"pseudo\" because we will obviate the type of actual pooling that the authors proposed by then, or the gaussian connections, or the `Tanh` activations. We will instead use `MaxPooling`s, `Fully connected` layers all the way through, and `ReLU` activations.\n","\n","For reference, the LeNet model is the following:\n","![](https://miro.medium.com/max/2154/1*1TI1aGBZ4dybR6__DI9dzA.png)"]},{"cell_type":"markdown","metadata":{"id":"1K_BP1dKiC28"},"source":["### Exercise 2\n","\n","Make the `ConvBlock` class to properly do: `Conv2d`, `ReLU`, and `MaxPool2d`. Ensure that for an input of size `1x32x32` you obtain an output feature map of size `6x14x14` as shown in the figure above for layer `S2`."]},{"cell_type":"code","metadata":{"id":"NoKiH2MZcLrH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633606837473,"user_tz":-120,"elapsed":211,"user":{"displayName":"Aleix Lahoz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5UYm5Q8-haFP2moJt24YYH3VUqATN22jr_XBbrw=s64","userId":"17486275261500353019"}},"outputId":"1a1881d2-a8c0-47c3-94e5-65e052a1e00a"},"source":["class ConvBlock(nn.Module):\n","\n","  def __init__(self, num_inp_channels, num_out_fmaps,\n","               kernel_size, pool_size=2):\n","    super().__init__()\n","    # TODO: define the 3 modules needed\n","    self.conv = nn.Conv2d(num_inp_channels,num_out_fmaps, kernel_size) #(input channel, output channel, kernel)\n","    self.relu = nn.ReLU()\n","    self.maxpool = nn.MaxPool2d(kernel_size=pool_size)\n","\n","  def forward(self, x):\n","    return self.maxpool(self.relu(self.conv(x)))\n","\n","x = torch.randn(1, 1, 32, 32)\n","y = ConvBlock(1, 6, 5, 2)(x)\n","assert y.shape[1] == 6, 'The amount of feature maps is not correct!'\n","assert y.shape[2] == 14 and y.shape[3] == 14, 'The spatial dimensions are not correct!'\n","print('Input shape: {}'.format(x.shape))\n","print('ConvBlock output shape (S2 level in Figure): {}'.format(y.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input shape: torch.Size([1, 1, 32, 32])\n","ConvBlock output shape (S2 level in Figure): torch.Size([1, 6, 14, 14])\n"]}]},{"cell_type":"markdown","metadata":{"id":"9nw1fBFGjPfE"},"source":["### Exercise 3\n","\n","Finish the `PseudoLeNet` class by including the following:\n","1. As the input images from MNIST are 28x28, add padding to make them 32x32 with the `torch.nn.ConstantPad2d` (https://pytorch.org/docs/stable/nn.html#torch.nn.ConstantPad2d).\n","2. Build the `mlp` classifier as a `nn.Sequential` stack of fully connected layers and ReLU activations, with the sizes shown in the figure above: [120, 84, 10]. Plug the appropriate output activation in the end to do multi-class classification.\n","3. Remember to \"flatten\" the feature maps coming out of the second `ConvBlock` and connect them to the output `mlp` to build the classifier in the `forward` function. This has to be done because fully connected layers (`Linear`) only accept features without any spatial dimension. Hence, all these spatial dimensions and channels are unrolled into single vectors, one per batch sample. **HINT: Remember the `.view()` operator to change tensors shape!**"]},{"cell_type":"code","metadata":{"id":"IoIWH2K7cLrH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633608011258,"user_tz":-120,"elapsed":224,"user":{"displayName":"Aleix Lahoz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5UYm5Q8-haFP2moJt24YYH3VUqATN22jr_XBbrw=s64","userId":"17486275261500353019"}},"outputId":"be7c84a0-7156-4bc6-de1f-ab7a68d9e2d9"},"source":["class PseudoLeNet(nn.Module):\n","\n","  def __init__(self):\n","    super().__init__()\n","    # TODO: Define the zero-padding\n","    self.pad = nn.ConstantPad2d([2,2,2,2],0)\n","\n","    self.conv1 = ConvBlock(1, 6, 5)\n","    self.conv2 = ConvBlock(6, 16, 5)\n","\n","    # TODO: Define the MLP at the deepest layers\n","    self.mlp = nn.Sequential(\n","      nn.Linear(16*5*5,120),\n","      nn.ReLU(),\n","      nn.Linear(120, 84),\n","      nn.ReLU(),\n","      nn.Linear(84, 10),\n","      nn.LogSoftmax(dim=1)\n","    )\n","\n","  def forward(self, x):\n","    x = self.pad(x)\n","    x = self.conv1(x)\n","    x = self.conv2(x)\n","    # Obtain the parameters of the tensor in terms of:\n","    # 1) batch size\n","    # 2) number of channels\n","    # 3) spatial \"height\"\n","    # 4) spatial \"width\"\n","    bsz, nch, height, width = x.shape\n","    # TODO: Flatten the feature map with the view() operator\n","    # within each batch sample\n","    x = x.view(bsz,nch*height*width)\n","    y = self.mlp(x)\n","    return y\n","\n","# Let's forward a toy example emulating the MNIST image size\n","plenet = PseudoLeNet()\n","y = plenet(torch.randn(1, 1, 28, 28))\n","print(y.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 10])\n"]}]},{"cell_type":"markdown","metadata":{"id":"3mbDGLTbpbRl"},"source":["### Now it's time to train and test the model.\n","\n","We take the previous training and test codes from MLP lab models and run them straightaway. Execute the cells below without more hessitation."]},{"cell_type":"code","metadata":{"id":"IlxCKY4GcLrI"},"source":["def correct_predictions(predicted_batch, label_batch):\n","  pred = predicted_batch.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n","  acum = pred.eq(label_batch.view_as(pred)).sum().item()\n","  return acum\n","\n","def train_epoch(train_loader, network, optimizer, criterion, hparams):\n","  # Activate the train=True flag inside the model\n","  network.train()\n","  device = hparams['device']\n","  avg_loss = None\n","  avg_weight = 0.1\n","  for batch_idx, (data, target) in enumerate(train_loader):\n","      data, target = data.to(device), target.to(device)\n","      optimizer.zero_grad()\n","      output = network(data)\n","      loss = criterion(output, target)\n","      loss.backward()\n","      if avg_loss:\n","        avg_loss = avg_weight * loss.item() + (1 - avg_weight) * avg_loss\n","      else:\n","        avg_loss = loss.item()\n","      optimizer.step()\n","      if batch_idx % hparams['log_interval'] == 0:\n","          print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","              epoch, batch_idx * len(data), len(train_loader.dataset),\n","              100. * batch_idx / len(train_loader), loss.item()))\n","  return avg_loss\n","\n","def test_epoch(test_loader, network, hparams):\n","    network.eval()\n","    device = hparams['device']\n","    test_loss = 0\n","    acc = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = network(data)\n","            test_loss += criterion(output, target, reduction='sum').item() # sum up batch loss\n","            # compute number of correct predictions in the batch\n","            acc += correct_predictions(output, target)\n","    # Average acc across all correct predictions batches now\n","    test_loss /= len(test_loader.dataset)\n","    test_acc = 100. * acc / len(test_loader.dataset)\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, acc, len(test_loader.dataset), test_acc,\n","        ))\n","    return test_loss, test_acc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d2Wg6K1wcLrI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633608322643,"user_tz":-120,"elapsed":209361,"user":{"displayName":"Aleix Lahoz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5UYm5Q8-haFP2moJt24YYH3VUqATN22jr_XBbrw=s64","userId":"17486275261500353019"}},"outputId":"892b7fa2-9f9a-4c4a-ba15-02ccdb442915"},"source":["tr_losses = []\n","te_losses = []\n","te_accs = []\n","network = PseudoLeNet()\n","network.to(hparams['device'])\n","optimizer = optim.RMSprop(network.parameters(), lr=hparams['learning_rate'])\n","criterion = F.nll_loss\n","\n","for epoch in range(1, hparams['num_epochs'] + 1):\n","  tr_losses.append(train_epoch(train_loader, network, optimizer, criterion, hparams))\n","  te_loss, te_acc = test_epoch(test_loader, network, hparams)\n","  te_losses.append(te_loss)\n","  te_accs.append(te_acc)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.309802\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.118506\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.119967\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.144038\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.121375\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.066600\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.185412\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.070086\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.117694\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.093126\n","\n","Test set: Average loss: 0.0605, Accuracy: 9801/10000 (98%)\n","\n","Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.018040\n","Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.015017\n","Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.012569\n","Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.096220\n","Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.015434\n","Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.007970\n","Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.037477\n","Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.042701\n","Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.030019\n","Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.067890\n","\n","Test set: Average loss: 0.0426, Accuracy: 9856/10000 (99%)\n","\n","Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.127096\n","Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.011225\n","Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.050151\n","Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.005880\n","Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.028072\n","Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.006297\n","Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.045187\n","Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.009029\n","Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.012833\n","Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.049065\n","\n","Test set: Average loss: 0.0353, Accuracy: 9892/10000 (99%)\n","\n","Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.006965\n","Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.019830\n","Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.017588\n","Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.005030\n","Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.041579\n","Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.006111\n","Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.018170\n","Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.065525\n","Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.003737\n","Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.011683\n","\n","Test set: Average loss: 0.0435, Accuracy: 9865/10000 (99%)\n","\n","Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.007652\n","Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.002713\n","Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.000731\n","Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.002459\n","Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.008246\n","Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.013884\n","Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.141880\n","Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.084174\n","Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.002415\n","Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.007309\n","\n","Test set: Average loss: 0.0486, Accuracy: 9867/10000 (99%)\n","\n","Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.017427\n","Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.097711\n","Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.007992\n","Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.003924\n","Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.000429\n","Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.000350\n","Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.078081\n","Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.024321\n","Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.054269\n","Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.055870\n","\n","Test set: Average loss: 0.0349, Accuracy: 9904/10000 (99%)\n","\n","Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.000572\n","Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.003121\n","Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.005015\n","Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.097597\n","Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.000172\n","Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.073448\n","Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.015318\n","Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.055250\n","Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.012539\n","Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.003821\n","\n","Test set: Average loss: 0.0338, Accuracy: 9897/10000 (99%)\n","\n","Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.012980\n","Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.001382\n","Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.016328\n","Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.001121\n","Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.006138\n","Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.003393\n","Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.028890\n","Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.000059\n","Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.004109\n","Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.002124\n","\n","Test set: Average loss: 0.0383, Accuracy: 9899/10000 (99%)\n","\n","Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.001651\n","Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.054291\n","Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.000529\n","Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.000189\n","Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.000532\n","Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.006050\n","Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.002738\n","Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.018881\n","Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.002486\n","Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.004822\n","\n","Test set: Average loss: 0.0481, Accuracy: 9872/10000 (99%)\n","\n","Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.009960\n","Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.010559\n","Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.000541\n","Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.027338\n","Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.000310\n","Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.000334\n","Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.001754\n","Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.002919\n","Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.001510\n","Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.004340\n","\n","Test set: Average loss: 0.0542, Accuracy: 9874/10000 (99%)\n","\n"]}]},{"cell_type":"code","metadata":{"id":"kgHAWj20cLrI","colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"status":"error","timestamp":1633639842470,"user_tz":-120,"elapsed":798,"user":{"displayName":"Aleix Lahoz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5UYm5Q8-haFP2moJt24YYH3VUqATN22jr_XBbrw=s64","userId":"17486275261500353019"}},"outputId":"5bb9a916-2092-423d-a827-e29453b51814"},"source":["plt.figure(figsize=(10, 8))\n","plt.subplot(2,1,1)\n","plt.xlabel('Epoch')\n","plt.ylabel('NLLLoss')\n","plt.plot(tr_losses, label='train')\n","plt.plot(te_losses, label='test')\n","plt.legend()\n","plt.subplot(2,1,2)\n","plt.xlabel('Epoch')\n","plt.ylabel('Test Accuracy [%]')\n","plt.plot(te_accs)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-49b7070725dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NLLLoss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"0P6TdeQGpnnP"},"source":["### The final result should be slightly above 99%, better than the MLP model for a comparable amount of training."]}]}