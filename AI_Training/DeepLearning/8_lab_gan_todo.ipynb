{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/telecombcn-dl/labs-all/blob/master/labs/gan/lab_gan_todo.ipynb","timestamp":1637841817267}]}},"cells":[{"cell_type":"markdown","metadata":{"id":"MbRVQ73oRVAO"},"source":["# DCGAN + conditional DCGAN\n","\n","\n","In this notebook will learn about Generative Adversarial Networks by implementing a DCGAN (Deep Convolutional GAN) to generate images from noise, followed by a conditional DCGAN.\n","\n","**Important:** Set the Colab environment to run on GPU\n"]},{"cell_type":"code","metadata":{"id":"Om9umswj-YFT"},"source":["import torch\n","from torch import nn, optim\n","from torchvision import transforms, datasets, utils\n","from PIL import Image\n","import numpy as np\n","import math\n","from IPython.display import display\n","from tqdm import tqdm\n","device = torch.device(\"cuda\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S_wpixTJ9fde"},"source":["## Hyperparameters"]},{"cell_type":"code","metadata":{"id":"f-AS4Nmq-YFU"},"source":["num_epochs = 20\n","\n","lr = 0.0002\n","betas = (0.5, 0.999)\n","\n","noise_size = 100\n","batch_size = 128\n","num_val_samples = 25\n","num_classes = 10\n","num_input_channels = 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P1AaRIWR9L5m"},"source":["## Dataset\n","Download and prepare dataset\n"]},{"cell_type":"code","metadata":{"id":"UV2TsJai-YFV"},"source":["train_transforms = transforms.Compose(\n","            [\n","                transforms.Resize(32),\n","                transforms.ToTensor(),\n","                transforms.Normalize((0.5,), (.5,))\n","            ])\n","dataset = datasets.MNIST(root='data', train=True, transform=train_transforms, download=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QDFqjh4T9a7W"},"source":["## Data Loader\n","Create a data loader for the MNIST dataset"]},{"cell_type":"code","metadata":{"id":"xqIYiKBz-YFW"},"source":["dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"--drGQMaGeop"},"source":["# DCGAN"]},{"cell_type":"markdown","metadata":{"id":"MFEBhsZb96T7"},"source":["## Networks\n","First, lets define our simple generator"]},{"cell_type":"markdown","metadata":{"id":"dD8ms_VzNmWn"},"source":["### Exercise 1: Generator\n","\n","The generator takes random noise as input and gives an image as output. Your exercise is to create the generator model.\n","\n","It should follow these guidelines:\n","* The input will be a vector with random noise of size `noise_size`\n","* You should first apply a fully connected with output size 512\\*4\\*4 (channels\\*height\\*width)\n","* Then you should apply 3 blocks of:\n","    * TransposedConvolution with kernel size 4, stride 2 and padding 1, and bias=False. For the first 2 blocks, the output channels should be 256 and 128. For the third block, the output channels should be the correct one to generate images of the dataset.\n","    * BatchNorm2d except for the last block.\n","    * ReLU activation for the first 2 blocks and Tanh for the third block.\n","\n","**Hint**: Remember to use reshape where necessary"]},{"cell_type":"code","metadata":{"id":"jO6mHMeU-YFY"},"source":["class Generator(torch.nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","        # TODO: Create the Fully connected layer using nn.Linear\n","        self.fc = nn.Linear(noise_size,512*4*4)\n","        # TODO: Create the First block using nn.Sequential with ConvTranspose2d, BatchNorm2d and activation\n","        self.convt1 = nn.Sequential(nn.ConvTranspose2d(512,256,kernel_size=4,stride=2,padding=1,bias=False),\n","                                    nn.BatchNorm2d(256),\n","                                    nn.ReLU())\n","        # TODO: Create the Second block using nn.Sequential with ConvTranspose2d, BatchNorm2d and activation\n","        self.convt2 = nn.Sequential(nn.ConvTranspose2d(256,128,kernel_size=4,stride=2,padding=1,bias=False),\n","                                    nn.BatchNorm2d(128),\n","                                    nn.ReLU())\n","        # TODO: Create the Third block using nn.Sequential with ConvTranspose2d, and activation\n","        self.convt3 = nn.Sequential(nn.ConvTranspose2d(128,num_input_channels,kernel_size=4,stride=2,padding=1,bias=False),\n","                                    nn.Tanh())\n","\n","    def forward(self, x):\n","        # TODO: Define the forward of the network, x is a random noise, it should be forwarded through the fully connected, then reshaped and finally forwarded throught the conv layers\n","        x = self.fc(x)\n","        x=  x.reshape(-1,512,4,4)#batch size, channels, height, width\n","        x = self.convt1(x)\n","        x = self.convt2(x)\n","        x = self.convt3(x)\n","\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LnSOu07Q-FLh"},"source":["Similarly lets define a simple discriminator"]},{"cell_type":"markdown","metadata":{"id":"9BQFM-vIPHvj"},"source":["### Exercise 2: Discriminator\n","\n","The discriminator takes an image as input and classifies it between Real or Fake (1 or 0). Your exercise is to create the discriminator model.\n","\n","It should follow these guidelines:\n","* The input will be an image of size `[num_input_channels, 32, 32]`\n","* You should apply 3 blocks of:\n","    * Convolution with kernel size 4, stride 2 and padding 1. The output channels should be 128, 256 and 512.\n","    * BatchNorm2d except for the first block.\n","    * LeakyReLU activation (alpha=0.2)\n","* Then you should apply a fully connected with input size 512\\*4\\*4 (channels\\*height\\*width) and the correct output size and activation for binary classification\n","\n","\n","**Hint**: Remember to use reshape/flatten where necessary"]},{"cell_type":"code","metadata":{"id":"bf2d8sqx-YFa"},"source":["class Discriminator(torch.nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","        # TODO: Create the First block using nn.Sequential with Conv2d and activation\n","        self.conv1 = nn.Sequential(nn.Conv2d(num_input_channels,128,kernel_size=4,stride=2,padding=1),\n","                                   nn.LeakyReLU())\n","        # TODO: Create the Second block using nn.Sequential with Conv2d, BatchNorm2d and activation\n","        self.conv2 = nn.Sequential(nn.Conv2d(128,256,kernel_size=4,stride=2,padding=1),\n","                                   nn.BatchNorm2d(256),\n","                                   nn.LeakyReLU(negative_slope=0.2))\n","        # TODO: Create the third block using nn.Sequential with Conv2d, BatchNorm2d and activation\n","        self.conv3 = nn.Sequential(nn.Conv2d(256,512,kernel_size=4,stride=2,padding=1),\n","                                   nn.BatchNorm2d(512),\n","                                   nn.LeakyReLU(negative_slope=0.2))\n","        # TODO: Create the fully connected block using nn.Sequential with Linear and activation\n","        self.fc = nn.Sequential(nn.Linear(512*4*4,1),\n","                                nn.Sigmoid())\n","\n","\n","    def forward(self, x):\n","        # TODO: Define the forward of the network, x is an image [num_input_channels, 32, 32], it should be forwarded through the conv layers, the flattened and forwarded through the fully connected\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        x = self.conv3(x)\n","        x = x.flatten(start_dim=1) #flatten will start in the 1st imagee\n","        x = self.fc(x)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v_Af1s2w-YFb"},"source":["generator = Generator().to(device)\n","optimizer_g = torch.optim.Adam(generator.parameters(), lr=lr, betas=betas)\n","\n","discriminator = Discriminator().to(device)\n","optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=betas)\n","\n","criterion = nn.BCELoss()\n","\n","def init_weights(m):\n","    if type(m) in {nn.Conv2d, nn.ConvTranspose2d, nn.Linear}:\n","        torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n","        if m.bias != None:\n","            torch.nn.init.constant_(m.bias, 0.0)\n","    if type(m) == nn.BatchNorm2d:\n","        nn.init.normal_(m.weight, 1.0, 0.02)\n","        nn.init.constant_(m.bias, 0)\n","\n","generator.apply(init_weights)\n","discriminator.apply(init_weights);\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VNKQKA4FfCL7"},"source":["## Train function"]},{"cell_type":"markdown","metadata":{"id":"ryDTioLaSd_t"},"source":["### Exercise 3: Train\n","\n","Complete the code. Take into account which labels should be used at each step of the training."]},{"cell_type":"code","metadata":{"id":"FPPB-eOF-YFb"},"source":["def train_batch(real_samples, generator, discriminator, optimizer_g, optimizer_d):\n","\n","    generator.train()\n","    discriminator.train()\n","\n","    current_batch_size = real_samples.shape[0]\n","    # TODO: Define the labels for the real and fake images so the discriminator can learn, of size [batch_size,1]\n","    label_real = torch.ones((batch_size,1)).to(device)\n","    label_fake = torch.zeros((batch_size,1)).to(device)\n","\n","    ####################\n","    # OPTIMIZE GENERATOR\n","    ####################\n","\n","    # TODO: Reset gradients\n","    optimizer_g.zero_grad()\n","\n","    # TODO: Generate fake samples\n","    input_noise = torch.randn((batch_size, noise_size)).to(device)\n","    fake_samples=generator(input_noise)\n","    # TODO: Evaluate the generated samples with the discriminator\n","    print(fake_samples)\n","    predictions_g_fake = discriminator(fake_samples)\n","    # Calculate error with respect to what the generator wants\n","    print(type(predictions_g_fake), type(label_real))\n","    loss_g = criterion(predictions_g_fake,label_real)\n","\n","    # TODO: Backpropagate\n","    loss_g.backward()\n","\n","    # TODO: Update weights (do a step in the optimizer)\n","    optimizer_g.step()\n","\n","    ####################\n","    # OPTIMIZE DISCRIMINATOR\n","    ####################\n","\n","    fake_samples = fake_samples.detach() #Let's detach them so they can be forwarded though the discriminator model\n","    # TODO: Reset gradients\n","    optimizer_d.zero_grad()\n","\n","    # TODO: Calculate discriminator prediction for real samples\n","    predictions_d_real = real_samples\n","\n","    # TODO: Calculate error with respect to what the discriminator wants\n","    loss_d_real = criterion(predictions_d_real,label_real)\n","\n","    # TODO: Calculate discriminator loss for fake samples\n","    predictions_d_fake = discriminator(fake_samples)\n","\n","    # TODO: Calculate error with respect to what the discriminator wants\n","    loss_d_fake = criterion(predictions_d_fake, label_fake)\n","\n","    # Total discriminator loss\n","    loss_d = (loss_d_real + loss_d_fake) / 2\n","    # TODO: Backpropagate\n","    loss_d.backward()\n","\n","    # TODO: Update weights (do a step in the optimizer)\n","    optimizer_d.step()\n","\n","    return loss_g.item(), loss_d.item()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VbBC3H3j0Yal"},"source":["## Evaluation function"]},{"cell_type":"code","metadata":{"id":"Qv4OyiUX-YFc"},"source":["@torch.no_grad()\n","def evaluate(generator, z_val):\n","    generator.eval()\n","    fake_samples = generator(z_val).cpu()\n","    # select a sample or create grid if img is a batch\n","    nrows = int(math.sqrt(fake_samples.shape[0]))\n","    img = utils.make_grid(fake_samples, nrow=nrows)\n","\n","    # unnormalize\n","    img = (img*0.5 + 0.5)*255\n","\n","    # to numpy\n","    image_numpy = img.numpy().astype(np.uint8)\n","    image_numpy = np.transpose(image_numpy, (1, 2, 0))\n","    return Image.fromarray(image_numpy)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iWS0uvRJEHGd"},"source":["## Train loop"]},{"cell_type":"code","metadata":{"id":"vyZSSimO-YFc","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1637845389926,"user_tz":-60,"elapsed":237,"user":{"displayName":"Aleix Lahoz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5UYm5Q8-haFP2moJt24YYH3VUqATN22jr_XBbrw=s64","userId":"17486275261500353019"}},"outputId":"eba27eda-ef69-4082-97d2-c3ae5515cc54"},"source":["z_val = torch.randn(num_val_samples, noise_size, device=device)\n","\n","for epoch in range(num_epochs):\n","\n","    for i, (real_samples, labels) in enumerate(dataloader):\n","        real_samples = real_samples.to(device)\n","        loss_g, loss_d = train_batch(real_samples, generator, discriminator, optimizer_g, optimizer_d)\n","\n","        if i % 100 == 0:\n","            fake_images = evaluate(generator, z_val)\n","            display(fake_images)\n","\n","            # Show current loss\n","            print(f\"epoch: {epoch+1}/{num_epochs} batch: {i+1}/{len(dataloader)} G_loss: {loss_g}, D_loss: {loss_d}\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[[-0.0100,  0.0079, -0.0727,  ..., -0.1292,  0.1149, -0.0418],\n","          [ 0.1523,  0.1335,  0.4080,  ...,  0.2066,  0.1523, -0.0972],\n","          [ 0.1585, -0.0184,  0.0206,  ...,  0.0888,  0.0336, -0.0150],\n","          ...,\n","          [ 0.0923,  0.1345,  0.0869,  ..., -0.3946, -0.0146, -0.1077],\n","          [-0.0203, -0.1255,  0.1405,  ...,  0.0340,  0.0675,  0.0062],\n","          [-0.0572,  0.0892, -0.1064,  ..., -0.1321, -0.0578, -0.0334]]],\n","\n","\n","        [[[-0.0183, -0.0367,  0.0586,  ..., -0.0362, -0.0260, -0.0440],\n","          [-0.0193, -0.2435, -0.1859,  ..., -0.0048, -0.0739, -0.0395],\n","          [ 0.1784,  0.2512, -0.1639,  ..., -0.1454,  0.2857,  0.1563],\n","          ...,\n","          [ 0.1722, -0.1845,  0.0602,  ...,  0.0731,  0.0115, -0.1452],\n","          [ 0.1340, -0.0215,  0.1173,  ..., -0.1301,  0.0391,  0.0919],\n","          [-0.0428, -0.2568,  0.0300,  ..., -0.0883,  0.0587, -0.0830]]],\n","\n","\n","        [[[ 0.0631, -0.0146, -0.1171,  ..., -0.1210,  0.1282,  0.0295],\n","          [ 0.1727, -0.0802,  0.2862,  ...,  0.0115,  0.3441,  0.0157],\n","          [ 0.0716,  0.1264,  0.3145,  ...,  0.0078, -0.1657,  0.0332],\n","          ...,\n","          [ 0.1441, -0.1087,  0.2557,  ...,  0.0597,  0.0191, -0.0274],\n","          [ 0.0374,  0.1974,  0.0449,  ...,  0.0431,  0.1291, -0.0267],\n","          [-0.0022, -0.0812,  0.0352,  ..., -0.1431,  0.0656, -0.0608]]],\n","\n","\n","        ...,\n","\n","\n","        [[[-0.1039, -0.0189, -0.0234,  ..., -0.0497,  0.0363, -0.0395],\n","          [ 0.2461,  0.3721,  0.4972,  ...,  0.0510,  0.4304, -0.0888],\n","          [ 0.1297,  0.2214,  0.0434,  ..., -0.0935,  0.2517, -0.1306],\n","          ...,\n","          [-0.1344,  0.0592,  0.0963,  ...,  0.0103, -0.2477, -0.0155],\n","          [ 0.0944,  0.0812,  0.0052,  ..., -0.2678, -0.0938,  0.0938],\n","          [-0.0141, -0.0840, -0.0154,  ..., -0.1373, -0.1690, -0.0786]]],\n","\n","\n","        [[[-0.0334,  0.0427,  0.1675,  ..., -0.0588, -0.0479, -0.0173],\n","          [ 0.1304,  0.1203, -0.0938,  ..., -0.0098,  0.0977,  0.0121],\n","          [ 0.0873, -0.1142,  0.2262,  ..., -0.0983,  0.1725,  0.0422],\n","          ...,\n","          [ 0.0483, -0.1373, -0.1836,  ..., -0.1049, -0.1783, -0.0658],\n","          [-0.1359, -0.3300, -0.1614,  ..., -0.1085, -0.1465,  0.1158],\n","          [-0.0654, -0.1839,  0.0858,  ...,  0.0535, -0.0955, -0.0978]]],\n","\n","\n","        [[[-0.0375,  0.0494, -0.0240,  ..., -0.1578, -0.0134, -0.0256],\n","          [ 0.2199,  0.0865,  0.1713,  ..., -0.0705,  0.0368,  0.0108],\n","          [ 0.0288, -0.0811,  0.2247,  ..., -0.2090,  0.0308, -0.0262],\n","          ...,\n","          [ 0.1684, -0.1269,  0.1576,  ..., -0.2005,  0.0444, -0.1070],\n","          [-0.1210, -0.2596, -0.0243,  ...,  0.0290,  0.2579,  0.0347],\n","          [-0.0210, -0.0583, -0.0408,  ..., -0.1662,  0.0207, -0.0578]]]],\n","       device='cuda:0', grad_fn=<TanhBackward0>)\n","<class 'NoneType'> <class 'torch.Tensor'>\n"]},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-66-5c899f63982b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreal_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mreal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mloss_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-64-d424b5282ccf>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(real_samples, generator, discriminator, optimizer_g, optimizer_d)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Calculate error with respect to what the generator wants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_g_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_real\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mloss_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_g_fake\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_real\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# TODO: Backpropagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2903\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2904\u001b[0m         \u001b[0mreduction_enum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2905\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2906\u001b[0m         raise ValueError(\n\u001b[1;32m   2907\u001b[0m             \u001b[0;34m\"Using a target size ({}) that is different to the input size ({}) is deprecated. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'size'"]}]},{"cell_type":"markdown","metadata":{"id":"0FgkBXj-9Jgq"},"source":["# Extra: Conditional GAN"]},{"cell_type":"markdown","metadata":{"id":"YdXFJ_B3E0l1"},"source":["## Networks"]},{"cell_type":"markdown","metadata":{"id":"bbf_CtNvS2uT"},"source":["### Exercise 4: Generator\n","\n","We will now modify the generator from before to a conditional generator. To do it, we will concatenated the input to the convolutions with an embedding of the label we want to generate.\n","\n","Complete the forward method. To do it, use the embedding layer with the label, and then use `torch.cat` to concatenate the label as a channel (after the corresponding `reshape`)\n","\n","**Hint**: The embedding is concatenated as a new channel."]},{"cell_type":"code","metadata":{"id":"24_D01Cp-YFd"},"source":["class ConditionalGenerator(torch.nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","        # TODO: Create the Fully connected layer using nn.Linear\n","        self.fc =...\n","\n","        self.embedding = nn.Embedding(num_classes, 4*4)\n","\n","        # TODO: Create the First block using nn.Sequential with ConvTranspose2d, BatchNorm2d and activation\n","        self.convt1 = nn.Sequential(...) #It is important that we add +1 to the first number of channels, since we have an extra channel that will be concatenated\n","        # TODO: Create the Second block using nn.Sequential with ConvTranspose2d, BatchNorm2d and activation\n","        self.convt2 = nn.Sequential(...)\n","        # TODO: Create the Third block using nn.Sequential with ConvTranspose2d, and activation\n","        self.convt3 = nn.Sequential(...)\n","\n","\n","    def forward(self, x, label):\n","        # TODO: Define the forward of the generator (first forward to the fc layer, reshape it, compute the embedding and concatenate it with x, and forward the convolutional layers)\n","        x = ... #First forward through the fully connected network\n","        x = ... #reshape it so it can be an image of [bs, 512,4,4]\n","\n","        emb = self.embedding(label).view(-1, 1, 4, 4) #This is done to be able to concatenate the lable with the noise image\n","        x = torch.cat([x, emb], dim=1) ## x = torch.cat(..., dim=1)\n","\n","        x = self.convt1(x) ## x = ... #now we can forward it though the 3 convolutional layers\n","        x = self.convt2(x) ## x = ...\n","        x = self.convt3(x) ## x = ...\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZhtPC283Tbkv"},"source":["### Exercise 5: Discriminator\n","\n","We will now modify the discriminator from before to a conditional discriminator. To do it, we will concatenated the input image with an embedding of the label we want to generate.\n","\n","Complete the forward method. To do it, use the embedding layer with the label, and then use `torch.cat` to concatenate the label as a channel (after the corresponding `reshape`)\n","\n","**Hint**: The embedding is concatenated as a new channel."]},{"cell_type":"code","metadata":{"id":"QhhWYyby-YFd"},"source":["class ConditionalDiscriminator(torch.nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.embedding = nn.Embedding(num_classes, 32*32)\n","\n","        # TODO: Create the First block using nn.Sequential with Conv2d and activation, remember to add +1 a the number of input channels\n","        self.conv1 = nn.Sequential(...)\n","        # TODO: Create the Second block using nn.Sequential with Conv2d, BatchNorm2d and activation\n","        self.conv2 = nn.Sequential(...)\n","        # TODO: Create the third block using nn.Sequential with Conv2d, BatchNorm2d and activation\n","        self.conv3 = nn.Sequential(...)\n","        # TODO: Create the fully connected block using nn.Sequential with Linear and activation\n","        self.fc = nn.Sequential(...)\n","\n","    def forward(self, x, label):\n","        # TODO: Define the forward of the discriminator (create the embedding and concatenate it with the images, forward though convolutional layers, flatten, and forward to the fc)\n","        x = torch.cat(...)\n","\n","        x = self.conv1(x) ## x = ... #now we can forward it though the 3 convolutional layers\n","        x = self.conv2(x) ## x = ...\n","        x = self.conv3(x) ## x = ...\n","        x = x.flatten(start_dim=1) ## x = ... #flatten to forward it to the fully connected\n","        x = self.fc(x) ## x = ... # now we can forward it though the fully connected\n","        return x\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8y4NYy8v-YFd"},"source":["generator = ConditionalGenerator().to(device)\n","optimizer_g = torch.optim.Adam(generator.parameters(), lr=lr, betas=betas)\n","\n","discriminator = ConditionalDiscriminator().to(device)\n","optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=betas)\n","\n","criterion = nn.BCELoss()\n","\n","def init_weights(m):\n","    if type(m) in {nn.Conv2d, nn.ConvTranspose2d, nn.Linear}:\n","        torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n","        if m.bias != None:\n","            torch.nn.init.constant_(m.bias, 0.0)\n","    if type(m) == nn.BatchNorm2d:\n","        nn.init.normal_(m.weight, 1.0, 0.02)\n","        nn.init.constant_(m.bias, 0)\n","\n","generator.apply(init_weights)\n","discriminator.apply(init_weights);\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"85_bncGME4VC"},"source":["## Train function"]},{"cell_type":"code","metadata":{"id":"3-J9X6Se-YFe"},"source":["def train_batch_conditional(real_samples, real_labels, generator, discriminator, optimizer_g, optimizer_d):\n","\n","    generator.train()\n","    discriminator.train()\n","    current_batch_size = real_samples.shape[0]\n","    # Define the labels for the real and fake images so the discriminator can learn, of size [batch_size,1]\n","    label_real = torch.ones(current_batch_size, 1, device=device)  ##label_real = torch...\n","    label_fake = torch.zeros(current_batch_size, 1, device=device) ##label_fake = torch...\n","\n","    ####################\n","    # OPTIMIZE GENERATOR\n","    ####################\n","\n","    # TODO: Reset gradients\n","    optimizer_g...\n","\n","    # TODO: Generate fake samples\n","    fake_labels = torch.randint(...) #we dfine the int to which we will condition the network\n","    fake_samples = ...\n","\n","    # TODO: Evaluate the generated samples with the discriminator\n","    predictions_g_fake = ...\n","\n","    # TODO: Calculate error with respect to what the generator wants\n","    predictions_g_fake = loss_g = criterion(...)\n","\n","    # TODO: Backpropagate\n","    loss_g.\n","\n","    # TODO: Update weights\n","    optimizer_g.\n","\n","    ####################\n","    # OPTIMIZE DISCRIMINATOR\n","    ####################\n","\n","    fake_samples = fake_samples.detach()\n","    # TODO: Reset gradients\n","    optimizer_d...\n","\n","    # TODO: Calculate discriminator prediction for real samples\n","    predictions_d_real = ...\n","\n","    # TODO: Calculate error with respect to what the discriminator wants\n","    predictions_d_real = loss_d_real = criterion(...)\n","\n","    # TODO: Calculate discriminator loss for fake samples\n","    predictions_d_fake = ...\n","\n","    # TODO: Calculate error with respect to what the discriminator wants\n","    loss_d_fake = loss_d_real = criterion(...)\n","\n","\n","    # Total discriminator loss\n","    loss_d = (loss_d_real + loss_d_fake) / 2\n","\n","    # TODO: Backpropagate\n","    loss_d...\n","\n","    # TODO: Update weights\n","    optimizer_d.\n","\n","    return loss_g.item(), loss_d.item()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7cZiJBr4E6P_"},"source":["## Evaluation function"]},{"cell_type":"code","metadata":{"id":"txrThUdQ-YFf"},"source":["@torch.no_grad()\n","def evaluate_conditional(generator, z_val, labels_val):\n","    generator.eval()\n","    fake_samples = generator(z_val, labels_val).cpu()\n","    # select a sample or create grid if img is a batch\n","    nrows = int(math.sqrt(fake_samples.shape[0]))\n","    img = utils.make_grid(fake_samples, nrow=nrows)\n","\n","    # unnormalize\n","    img = (img*0.5 + 0.5)*255\n","\n","    # to numpy\n","    image_numpy = img.numpy().astype(np.uint8)\n","    image_numpy = np.transpose(image_numpy, (1, 2, 0))\n","    return Image.fromarray(image_numpy)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w-g63Js1E9N7"},"source":["## Train loop"]},{"cell_type":"code","metadata":{"id":"f2AOexuv-YFf"},"source":["from itertools import cycle\n","\n","z_val = torch.randn(num_val_samples, noise_size, device=device)\n","labels_cycle = cycle(range(num_classes))\n","labels_val = torch.tensor([next(labels_cycle) for i in range(num_val_samples)], device=device).unsqueeze(1) #the labels will be a cycle from 0 to 9\n","for epoch in range(num_epochs):\n","\n","    for i, (real_samples, real_labels) in enumerate(dataloader):\n","        real_samples = real_samples.to(device)\n","        real_labels = real_labels.unsqueeze(1).to(device)\n","        loss_g, loss_d = train_batch_conditional(real_samples, real_labels, generator, discriminator, optimizer_g, optimizer_d)\n","\n","        if i % 100 == 0:\n","            fake_images = evaluate_conditional(generator, z_val, labels_val)\n","            display(fake_images)\n","\n","            # Show current loss\n","            print(f\"epoch: {epoch+1}/{num_epochs} batch: {i+1}/{len(dataloader)} G_loss: {loss_g}, D_loss: {loss_d}\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3JgI6aKa-YFf"},"source":["#You can play and visualize different numbers\n","number_chosen = 2\n","z_val = torch.randn(num_val_samples, noise_size, device=device)\n","number_chosen_torch = torch.tensor([number_chosen for i in range(num_val_samples)], device=device).unsqueeze(1)\n","\n","images_number_chosen = evaluate_conditional(generator, z_val, number_chosen_torch)\n","display(images_number_chosen)"],"execution_count":null,"outputs":[]}]}